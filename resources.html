<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Adarsh Kuthuru - Resources</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="resources.html" class="logo">Resources</a>
					</header> 

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li><a href="index.html">Projects</a></li>
							<li class="active"><a href="resources.html">Resources</a></li>
							<li><a href="about.html">About me</a></li>
						</ul>
						<ul class="icons">
							<li><a href="https://www.linkedin.com/in/adarshkp1/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
							<li><a href="https://github.com/adarshkuthuru?tab=repositories" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
								<header class="major">
									<span class="date">April 16, 2022</span>
									<h2>Solving non-convex optimization problems</h2>
									<h3>Explanation for why we can't use Neural Networks for non-convex optimization
										problems instead of Genetic Algorithms</h3>
								</header>
	
								<p>In order to choose any algorithm to solve a non-linear (non-convex) optimization problem, we need to consider the following factors:
									<ol>
									<li> How fast can the algorithm learn and converge? (Initialization matters a lot for this)</li>
									<li>  How to resolve if the solution gets stuck in local optima or saddle points?</li>
									<li>   Sensitivity of the algorithm to new data i.e., if the training data set is 
										not representative of the population, how would it perform with test data?</li>
									</ol> 
								</p>

								<center>
								<figure>
								<div class="image"><img src="images/nonconvex.png" alt="" width="500" height="400"/></div>
								<figcaption><a href="https://medium.com/swlh/non-convex-optimization-in-deep-learning-26fa30a2b2b3">fig.</a> Surface of a non-convex objective function </figcaption>
								</figure>
								</center>

								<!-- for space -->
								&nbsp; 
								&nbsp; 
								
								<!-- <h3>1. How fast can the algorithm learn and converge?</h3> -->
								<h3>Challenges when using Neural networks for non-convex optimizations</h3>
								<ol>
								<li> <b>Hyperparamter Optimization: </b> <br> In comparison to Genetic Algorithms, Neural Networks have more hyperparameters to be optimized
									 like learning rate (need to be specified for gradient descent approach), 
								number of hidden layers required, number of neurons/nodes required in each layer, 
								choice of activation functions etc. in addition to factors like the number of iterations/epochs required. 
								It could take a significant/polynomial time to optimize all of the hyperparameters. Therefore, it is at least an NP-Hard problem.
								</li>



								
								&nbsp; 
								<li><b>Poor conditioning of the cost function: </b> <br> Genetic Algorithms do not require the error terms (cost function) to be minimized. 
									They follow a heuristic approach wherein the solutions are randomly generated 
									through techniques like selection, crossover, and mutation which are unique to them, and substituted in the objective functions to search for global optima. 
									Whereas if Neural Networks are used in an optimization problem, they are required to fit a model and minimize the residual term. 
									This means that there is a possibility of poor conditioning of the cost function.  
								</li>
								
								&nbsp; 
								<p> Hessian matrices of cost functions, which are
									second-order partial derivatives of cost functions with respect to explanatory variables describe the local curvatures of the function along 
									the axes ot those variables.

									<center>
										<figure>
										<div class="image"><img src="images/hessian.png" alt="" width="250" height="200"/></div>
										<figcaption>fig. Hessian Matrix </figcaption>
										</figure>
									</center>


									Hence, hessian matrices are used to determine if a solution is a saddle point or not. But for neural networks, computing and storing full hessian matrix
									takes O(n<sup>2</sup>) memory, which is not feasible as they have complex cost functions. Hence, an approximation is 
									made using numerical methods like truncated-Newton algorithms, which lead to poor conditioning of errors. This could result in slower learning rates, rapid changes 
									in output for a small change in inputs etc., as the optimization process is not stable.
								</p>


								<li><b>Vanishing/Exploding Gradient problem:</b> <br> 
									In forward propagation, neural networks are initialized by assigning random weights to the nodes, transformed using an activation function, 
									and a non-linear model is fitted.
									In the process of backward propagation, the weights (slope) and the constant (intercept) are used to minimize the cost function as 
									per below formula and are updated at a pre-specified learning rate. This updation of weights approach is called gradient descent.<br> 

									&nbsp; 
									<center>
									<math>
										<b>W<sub>new</sub> = W<sub>old</sub> - α*<box>dL/dw</box> </b> <br> 
									

									<!-- Wnew = Wold – (α * dL/dw) -->

									where,

									W<sub>new</sub> = the new weight of explanatory variable X<sub>i</sub>; <br> 

									W<sub>old</sub> = the old weight of explanatory variable X<sub>i</sub>; <br> 

									α = learning rate; <br> 

									<box>dL/dw</box> is the partial derivative of the loss function for each of the Xs. 
										It is the rate of change of the loss function to the change in weight.

									</math>
									</center>

									<!-- <center>
										<figure>
										<div class="image"><img src="images/cost.jfif" alt="" width="200" height="50"/></div>
										<figcaption>fig. Gradient Descent formula </figcaption>
										</figure>
									</center> -->
								</li>

								&nbsp; 
								<p>
									Sometimes when this gradient approaches zero due to the characterstic of the activation functions or at saddle points and 
									local minima, the weights dont update. This is called vanishing gradient problem. When the gradient approaches infinity, 
									it is called exploding gradient problem. In sequential models like Recurrent Neural Networks, Long-short-term memory models (RNN/LSTM) 
									which are predominantly used for NLP, these mean 
									zero correlation of the current state with previous state and 100% correlation respectively.
								</p>

								<!-- &nbsp;  -->
								<center>
									<figure>
									<div class="animated-gif"><img src="images/Opt.gif" alt="" width="500" height="400"/></div>
									<figcaption><a href="https://pin.it/6E7hJn3">fig.</a> Learning and converging rates of some Neural network extensions that I will talk about below</figcaption>
									</figure>
								</center>
								<p>
									In order to address these various solutions were proposed like gradient clipping (where the gradients are constrained by a threshold), usage of 
									activation functions like Rectified Linear Units (ReLU), Stochastic Gradient Descent (SGD - adding a stochastic term to gradient descent which 
									updates weights randomly), Momentum, NAG, Adaptive Learning Rate, Adagrad, Adadelta, Rmsprop etc.
								</p>
							
							<ul>
								<center>
									<figure>
									<div class="image"><img src="images/SGD.png" alt="" width="550" height="300"/></div>
									<figcaption><a href="https://ankit-ai.blogspot.com/2018/11/optimization-algorithms-for-machine.html">fig.</a> Stochastic Gradient Descent Vs Gradient Descent </figcaption>
									</figure>
								</center>

								<li><b>Stochastic Gradient Descent:</b> <br> 
									From the above animation and image, SGD is still at the risk of getting stuck in local minima. So we can eliminate it as a possible solution.
		
								</li>

								<li><b>Stochastic Gradient Descent with Momentum:</b> <br> 
									In the presence of curvature, regular SGD oscillates a lot as shown in the below image, thus delaying the process of converging. Therefore SGD with momentum approach was 
									propsoed wherein the gradient is averaged to dampen the oscillations and obtain a faster path to optimization.
								</li>
								<center>
									<figure>
									<div class="image"><img src="images/mom.png" alt="" width="450" height="150"/></div>
									<figcaption><a href="https://mitpress.mit.edu/books/deep-learning">fig.</a> (Left) Regular SGD, (right) SGD with Momentum. <br>Goodfellow, I., Bengio, Y. and Courville, A. (2016). Deep learning. MIT press.</figcaption>
									</figure>
								</center>
								
								<li><b>Adaptive learning rate:</b> <br> 
									In the below image, when we are trying to minimize the cost function 'J' which is a function of parameters <math>Θ<sub>1</sub> & Θ<sub>2</sub></math>,
									we need to optimize the magnitude of oscillations to be proportional to the range of the parameter. Hence, the learning rates should be lower for lower oscillations and vice versa.
								</li>
								<center>
									<figure>
									<div class="image"><img src="images/adalearn.png" alt="" width="450" height="250"/></div>
									<figcaption><a href="https://towardsdatascience.com/neural-network-optimization-7ca72d4db3e0">fig.</a> Adaptive learning rates.</figcaption>
									</figure>
								</center>
							</ul>

							</ol>

							</section>


					</div>

				
				<!-- Footer -->
				<footer id="footer">
					<section class="split contact">
						<section class="alt">
							<h3>Address</h3>
							<p>61 Algonquin Rd,<br />
								Chestnut Hill, MA-02467</p>
						</section>
						<section>
							<h3>Email</h3>
							<p><a href="adarshkuthuru@gmail.com">adarshkuthuru@gmail.com</a></p>
						</section>
						<section>
							<h3>Social</h3>
							<ul class="icons alt">
								<li><a href="https://www.linkedin.com/in/adarshkp1/" class="icon brands alt fa-linkedin"><span class="label">LinkedIn</span></a></li>
								<li><a href="https://github.com/adarshkuthuru?tab=repositories" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
							</ul>
						</section>
					</section>
				</footer>

			<!-- Copyright -->
				<div id="copyright">
					<ul><li>&copy; adarshkuthuru</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li></ul>
				</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>