<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Aman's AI Journal • Coursera-DL • Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization</title>
  <meta name="viewport" content="width=device-width">
  <meta name="description" content="Aman's AI Journal | Course notes and learning material for Artificial Intelligence and Deep Learning Stanford classes.">
  <link rel="canonical" href="https://aman.ai/coursera-dl/improving-deep-neural-networks/">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/css/main.css">

  <!-- Google fonts -->
  <!-- <link href='https://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>-->

  <!-- RSS feed -->
  <link rel="alternate" type="application/atom+xml" title="Aman’s AI Journal" href="/feed.xml">  
  
  <link href="https://aman.ai/favicon.jpg" rel="shortcut icon" />

  <!-- Google ads -->
  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5905744527956213"
     crossorigin="anonymous"></script>
</head>


    <body>

      <script src="https://unpkg.com/vanilla-back-to-top@7.2.1/dist/vanilla-back-to-top.min.js"></script>
      <script>addBackToTop({
        backgroundColor: '#fff',
        innerHTML: 'Back to Top',
        textColor: '#333'
      })</script>
      <style>
        #back-to-top {
          border: 1px solid #ccc;
          border-radius: 0;
          font-family: sans-serif;
          font-size: 14px;
          width: 100px;
          text-align: center;
          line-height: 30px;
          height: 30px;
        }
      </style>   

    <header class="site-header">

  <a class="site-title" href="../">Distilled AI</a>

  <a class="site-link" href="https://aman.ai">Back to aman.ai</a>

  <!-- Html Elements for Search -->
  <div id="search-container">
  <input class="site-search-box" type="text" autocomplete="off" id="search-input" placeholder="search...">
  <div id="results-container"></div>
  </div>

  <!-- Script pointing to aman-script.js -->
  <script src="https://aman.ai/js/aman-search.min.js" type="text/javascript"></script>

  <!-- Configuration -->
  <script>
  document.getElementById('search-input').value='';
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('results-container'),
    exclude: ["cs231a"],
    searchResultTemplate: '<div class="site-search-results"><a href="{url}">{title}</a></div>',
    noResultsText: '<div class="site-search-results"><p>No results found</p></div>',
    json: 'https://aman.ai/search.json',
    limit: 5,
    fuzzy: false,
  })
  </script>    

</header>     

    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Coursera-DL • Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization</h1>
  </header>

  <article class="post-content">
  <ul id="markdown-toc">
  <li><a href="#course-summary" id="markdown-toc-course-summary">Course summary</a></li>
  <li><a href="#practical-aspects-of-deep-learning" id="markdown-toc-practical-aspects-of-deep-learning">Practical aspects of Deep Learning</a>    <ul>
      <li><a href="#train--dev--test-sets" id="markdown-toc-train--dev--test-sets">Train / Dev / Test sets</a></li>
      <li><a href="#bias--variance" id="markdown-toc-bias--variance">Bias / Variance</a></li>
      <li><a href="#basic-recipe-for-machine-learning" id="markdown-toc-basic-recipe-for-machine-learning">Basic Recipe for Machine Learning</a></li>
      <li><a href="#regularization" id="markdown-toc-regularization">Regularization</a></li>
      <li><a href="#why-regularization-reduces-overfitting" id="markdown-toc-why-regularization-reduces-overfitting">Why regularization reduces overfitting?</a></li>
      <li><a href="#dropout-regularization" id="markdown-toc-dropout-regularization">Dropout Regularization</a></li>
      <li><a href="#understanding-dropout" id="markdown-toc-understanding-dropout">Understanding Dropout</a></li>
      <li><a href="#other-regularization-methods" id="markdown-toc-other-regularization-methods">Other regularization methods</a></li>
      <li><a href="#normalizing-inputs" id="markdown-toc-normalizing-inputs">Normalizing inputs</a></li>
      <li><a href="#vanishing--exploding-gradients" id="markdown-toc-vanishing--exploding-gradients">Vanishing / Exploding gradients</a></li>
      <li><a href="#weight-initialization-for-deep-networks" id="markdown-toc-weight-initialization-for-deep-networks">Weight Initialization for Deep Networks</a></li>
      <li><a href="#numerical-approximation-of-gradients" id="markdown-toc-numerical-approximation-of-gradients">Numerical approximation of gradients</a></li>
      <li><a href="#gradient-checking-implementation-notes" id="markdown-toc-gradient-checking-implementation-notes">Gradient checking implementation notes</a></li>
      <li><a href="#initialization-summary" id="markdown-toc-initialization-summary">Initialization summary</a></li>
      <li><a href="#regularization-summary" id="markdown-toc-regularization-summary">Regularization summary</a>        <ul>
          <li><a href="#1-l2-regularization" id="markdown-toc-1-l2-regularization">1. L2 Regularization</a></li>
          <li><a href="#2-dropout" id="markdown-toc-2-dropout">2. Dropout</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#optimization-algorithms" id="markdown-toc-optimization-algorithms">Optimization algorithms</a>    <ul>
      <li><a href="#mini-batch-gradient-descent" id="markdown-toc-mini-batch-gradient-descent">Mini-batch gradient descent</a></li>
      <li><a href="#understanding-mini-batch-gradient-descent" id="markdown-toc-understanding-mini-batch-gradient-descent">Understanding mini-batch gradient descent</a></li>
      <li><a href="#exponentially-weighted-averages" id="markdown-toc-exponentially-weighted-averages">Exponentially weighted averages</a></li>
      <li><a href="#understanding-exponentially-weighted-averages" id="markdown-toc-understanding-exponentially-weighted-averages">Understanding exponentially weighted averages</a></li>
      <li><a href="#bias-correction-in-exponentially-weighted-averages" id="markdown-toc-bias-correction-in-exponentially-weighted-averages">Bias correction in exponentially weighted averages</a></li>
      <li><a href="#gradient-descent-with-momentum" id="markdown-toc-gradient-descent-with-momentum">Gradient descent with momentum</a></li>
      <li><a href="#rmsprop" id="markdown-toc-rmsprop">RMSprop</a></li>
      <li><a href="#adam-optimization-algorithm" id="markdown-toc-adam-optimization-algorithm">Adam optimization algorithm</a></li>
      <li><a href="#learning-rate-decay" id="markdown-toc-learning-rate-decay">Learning rate decay</a></li>
      <li><a href="#the-problem-of-local-optima" id="markdown-toc-the-problem-of-local-optima">The problem of local optima</a></li>
    </ul>
  </li>
  <li><a href="#hyperparameter-tuning-batch-normalization-and-programming-frameworks" id="markdown-toc-hyperparameter-tuning-batch-normalization-and-programming-frameworks">Hyperparameter tuning, Batch Normalization and Programming Frameworks</a>    <ul>
      <li><a href="#tuning-process" id="markdown-toc-tuning-process">Tuning process</a></li>
      <li><a href="#using-an-appropriate-scale-to-pick-hyperparameters" id="markdown-toc-using-an-appropriate-scale-to-pick-hyperparameters">Using an appropriate scale to pick hyperparameters</a></li>
      <li><a href="#hyperparameters-tuning-in-practice-pandas-vs-caviar" id="markdown-toc-hyperparameters-tuning-in-practice-pandas-vs-caviar">Hyperparameters tuning in practice: Pandas vs. Caviar</a></li>
      <li><a href="#normalizing-activations-in-a-network" id="markdown-toc-normalizing-activations-in-a-network">Normalizing activations in a network</a></li>
      <li><a href="#fitting-batch-normalization-into-a-neural-network" id="markdown-toc-fitting-batch-normalization-into-a-neural-network">Fitting Batch Normalization into a neural network</a></li>
      <li><a href="#why-does-batch-normalization-work" id="markdown-toc-why-does-batch-normalization-work">Why does Batch normalization work?</a></li>
      <li><a href="#batch-normalization-at-test-time" id="markdown-toc-batch-normalization-at-test-time">Batch normalization at test time</a></li>
      <li><a href="#softmax-regression" id="markdown-toc-softmax-regression">Softmax Regression</a></li>
      <li><a href="#training-a-softmax-classifier" id="markdown-toc-training-a-softmax-classifier">Training a Softmax classifier</a></li>
      <li><a href="#deep-learning-frameworks" id="markdown-toc-deep-learning-frameworks">Deep learning frameworks</a></li>
      <li><a href="#tensorflow" id="markdown-toc-tensorflow">TensorFlow</a></li>
    </ul>
  </li>
  <li><a href="#extra-notes" id="markdown-toc-extra-notes">Extra Notes</a></li>
</ul>

<h2 id="course-summary">Course summary</h2>

<p>Here are the course summary as its given on the course <a href="https://www.coursera.org/learn/deep-neural-network">link</a>:</p>

<blockquote>
  <p>This course will teach you the “magic” of getting deep learning to work well. Rather than the deep learning process being a black box, you will understand what drives performance, and be able to more systematically get good results. You will also learn TensorFlow.</p>

  <p>After 3 weeks, you will:</p>
  <ul>
    <li>Understand industry best-practices for building deep learning applications.</li>
    <li>Be able to effectively use the common neural network “tricks”, including initialization, L2 and dropout regularization, Batch normalization, gradient checking,</li>
    <li>Be able to implement and apply a variety of optimization algorithms, such as mini-batch gradient descent, Momentum, RMSprop and Adam, and check for their convergence.</li>
    <li>Understand new best-practices for the deep learning era of how to set up train/dev/test sets and analyze bias/variance</li>
    <li>Be able to implement a neural network in TensorFlow.</li>
  </ul>

  <p>This is the second course of the Deep Learning Specialization.</p>
</blockquote>

<h2 id="practical-aspects-of-deep-learning">Practical aspects of Deep Learning</h2>

<h3 id="train--dev--test-sets">Train / Dev / Test sets</h3>

<ul>
  <li>Its impossible to get all your hyperparameters right on a new application from the first time.</li>
  <li>So the idea is you go through the loop: <code class="language-plaintext highlighter-rouge">Idea ==&gt; Code ==&gt; Experiment</code>.</li>
  <li>You have to go through the loop many times to figure out your hyperparameters.</li>
  <li>Your data will be split into three parts:
    <ul>
      <li>Training set.       (Has to be the largest set)</li>
      <li>Hold-out cross validation set / Development or “dev” set.</li>
      <li>Testing set.</li>
    </ul>
  </li>
  <li>You will try to build a model upon training set then try to optimize hyperparameters on dev set as much as possible. Then after your model is ready you try and evaluate the testing set.</li>
  <li>so the trend on the ratio of splitting the models:
    <ul>
      <li>If size of the  dataset is 100 to 1000000  ==&gt; 60/20/20</li>
      <li>If size of the  dataset is 1000000  to INF  ==&gt; 98/1/1 or  99.5/0.25/0.25</li>
    </ul>
  </li>
  <li>The trend now gives the training data the biggest sets.</li>
  <li>Make sure the dev and test set are coming from the same distribution.
    <ul>
      <li>For example if cat training pictures is from the web and the dev/test pictures are from users cell phone they will mismatch. It is better to make sure that dev and test set are from the same distribution.</li>
    </ul>
  </li>
  <li>The dev set rule is to try them on some of the good models you’ve created.</li>
  <li>Its OK to only have a dev set without a testing set. But a lot of people in this case call the dev set as the test set. A better terminology is to call it a dev set as its used in the development.</li>
</ul>

<h3 id="bias--variance">Bias / Variance</h3>

<ul>
  <li>Bias / Variance techniques are Easy to learn, but difficult to master.</li>
  <li>So here the explanation of Bias / Variance:
    <ul>
      <li>If your model is underfitting (logistic regression of non linear data) it has a “high bias”</li>
      <li>If your model is overfitting then it has a “high variance”</li>
      <li>Your model will be alright if you balance the Bias / Variance</li>
      <li>For more:
<img src="/coursera-dl/assets/improving-deep-neural-networks/01-_Bias_-_Variance.png" alt="" /></li>
    </ul>
  </li>
  <li>Another idea to get the bias /  variance if you don’t have a 2D plotting mechanism:
    <ul>
      <li>High variance (overfitting) for example:
        <ul>
          <li>Training error: 1%</li>
          <li>Dev error: 11%</li>
        </ul>
      </li>
      <li>high Bias (underfitting) for example:
        <ul>
          <li>Training error: 15%</li>
          <li>Dev error: 14%</li>
        </ul>
      </li>
      <li>high Bias (underfitting) &amp;&amp; High variance (overfitting) for example:
        <ul>
          <li>Training error: 15%</li>
          <li>Test error: 30%</li>
        </ul>
      </li>
      <li>Best:
        <ul>
          <li>Training error: 0.5%</li>
          <li>Test error: 1%</li>
        </ul>
      </li>
      <li>These Assumptions came from that human has 0% error. If the problem isn’t like that you’ll need to use human error as baseline.</li>
    </ul>
  </li>
</ul>

<h3 id="basic-recipe-for-machine-learning">Basic Recipe for Machine Learning</h3>

<ul>
  <li>If your algorithm has a high bias:
    <ul>
      <li>Try to make your NN bigger (size of hidden units, number of layers)</li>
      <li>Try a different model that is suitable for your data.</li>
      <li>Try to run it longer.</li>
      <li>Different (advanced) optimization algorithms.</li>
    </ul>
  </li>
  <li>If your algorithm has a high variance:
    <ul>
      <li>More data.</li>
      <li>Try regularization.</li>
      <li>Try a different model that is suitable for your data.</li>
    </ul>
  </li>
  <li>You should try the previous two points until you have a low bias and low variance.</li>
  <li>In the older days before deep learning, there was a “Bias/variance tradeoff”. But because now you have more options/tools for solving the bias and variance problem its really helpful to use deep learning.</li>
  <li>Training a bigger neural network never hurts.</li>
</ul>

<h3 id="regularization">Regularization</h3>

<ul>
  <li>Adding regularization to NN will help it reduce variance (overfitting)</li>
  <li>L1 matrix norm:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">||W|| = Sum(|w[i,j]|)  # sum of absolute values of all w</code></li>
    </ul>
  </li>
  <li>L2 matrix norm because of arcane technical math reasons is called Frobenius norm:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">||W||^2 = Sum(|w[i,j]|^2)	# sum of all w squared</code></li>
      <li>Also can be calculated as <code class="language-plaintext highlighter-rouge">||W||^2 = W.T * W if W is a vector</code></li>
    </ul>
  </li>
  <li>Regularization for logistic regression:
    <ul>
      <li>The normal cost function that we want to minimize is: <code class="language-plaintext highlighter-rouge">J(w,b) = (1/m) * Sum(L(y(i),y'(i)))</code></li>
      <li>The L2 regularization version: <code class="language-plaintext highlighter-rouge">J(w,b) = (1/m) * Sum(L(y(i),y'(i))) + (lambda/2m) * Sum(|w[i]|^2)</code></li>
      <li>The L1 regularization version: <code class="language-plaintext highlighter-rouge">J(w,b) = (1/m) * Sum(L(y(i),y'(i))) + (lambda/2m) * Sum(|w[i]|)</code></li>
      <li>The L1 regularization version makes a lot of w values become zeros, which makes the model size smaller.</li>
      <li>L2 regularization is being used much more often.</li>
      <li><code class="language-plaintext highlighter-rouge">lambda</code> here is the regularization parameter (hyperparameter)</li>
    </ul>
  </li>
  <li>Regularization for NN:
    <ul>
      <li>
        <p>The normal cost function that we want to minimize is: <br />
<code class="language-plaintext highlighter-rouge">J(W1,b1...,WL,bL) = (1/m) * Sum(L(y(i),y'(i)))</code></p>
      </li>
      <li>
        <p>The L2 regularization version: <br />
<code class="language-plaintext highlighter-rouge">J(w,b) = (1/m) * Sum(L(y(i),y'(i))) + (lambda/2m) * Sum((||W[l]||^2)</code></p>
      </li>
      <li>
        <p>We stack the matrix as one vector <code class="language-plaintext highlighter-rouge">(mn,1)</code> and then we apply <code class="language-plaintext highlighter-rouge">sqrt(w1^2 + w2^2.....)</code></p>
      </li>
      <li>
        <p>To do back propagation (old way): <br />
<code class="language-plaintext highlighter-rouge">dw[l] = (from back propagation)</code></p>
      </li>
      <li>
        <p>The new way: <br />
<code class="language-plaintext highlighter-rouge">dw[l] = (from back propagation) + lambda/m * w[l]</code></p>
      </li>
      <li>
        <p>So plugging it in weight update step:</p>

        <ul>
          <li>
            <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>w[l] = w[l] - learning_rate * dw[l]
     = w[l] - learning_rate * ((from back propagation) + lambda/m * w[l])
     = w[l] - (learning_rate*lambda/m) * w[l] - learning_rate * (from back propagation) 
     = (1 - (learning_rate*lambda)/m) * w[l] - learning_rate * (from back propagation)
</code></pre></div>            </div>
          </li>
        </ul>
      </li>
      <li>
        <p>In practice this penalizes large weights and effectively limits the freedom in your model.</p>
      </li>
      <li>
        <p>The new term <code class="language-plaintext highlighter-rouge">(1 - (learning_rate*lambda)/m) * w[l]</code>  causes the <strong>weight to decay</strong> in proportion to its size.</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="why-regularization-reduces-overfitting">Why regularization reduces overfitting?</h3>

<p>Here are some intuitions:</p>
<ul>
  <li>Intuition 1:
    <ul>
      <li>If <code class="language-plaintext highlighter-rouge">lambda</code> is too large - a lot of w’s will be close to zeros which will make the NN simpler (you can think of it as it would behave closer to logistic regression).</li>
      <li>If <code class="language-plaintext highlighter-rouge">lambda</code> is good enough it will just reduce some weights that makes the neural network overfit.</li>
    </ul>
  </li>
  <li>Intuition 2 (with <em>tanh</em> activation function):
    <ul>
      <li>If <code class="language-plaintext highlighter-rouge">lambda</code> is too large, w’s will be small (close to zero) - will use the linear part of the <em>tanh</em> activation function, so we will go from non linear activation to <em>roughly</em> linear which would make the NN a <em>roughly</em> linear classifier.</li>
      <li>If <code class="language-plaintext highlighter-rouge">lambda</code> good enough it will just make some of <em>tanh</em> activations <em>roughly</em> linear which will prevent overfitting.</li>
    </ul>
  </li>
</ul>

<p><em><strong>Implementation tip</strong></em>: if you implement gradient descent, one of the steps to debug gradient descent is to plot the cost function J as a function of the number of iterations of gradient descent and you want to see that the cost function J decreases <strong>monotonically</strong> after every elevation of gradient descent with regularization. If you plot the old definition of J (no regularization) then you might not see it decrease monotonically.</p>

<h3 id="dropout-regularization">Dropout Regularization</h3>

<ul>
  <li>In most cases Andrew Ng tells that he uses the L2 regularization.</li>
  <li>The dropout regularization eliminates some neurons/weights on each iteration based on a probability.</li>
  <li>A most common technique to implement dropout is called “Inverted dropout”.</li>
  <li>
    <p>Code for Inverted dropout:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">keep_prob</span> <span class="o">=</span> <span class="mf">0.8</span>   <span class="c1"># 0 &lt;= keep_prob &lt;= 1
</span><span class="n">l</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># this code is only for layer 3
# the generated number that are less than 0.8 will be dropped. 80% stay, 20% dropped
</span><span class="n">d3</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">l</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">a</span><span class="p">[</span><span class="n">l</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">keep_prob</span>

<span class="n">a3</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">a3</span><span class="p">,</span><span class="n">d3</span><span class="p">)</span>   <span class="c1"># keep only the values in d3
</span>
<span class="c1"># increase a3 to not reduce the expected value of output
# (ensures that the expected value of a3 remains the same) - to solve the scaling problem
</span><span class="n">a3</span> <span class="o">=</span> <span class="n">a3</span> <span class="o">/</span> <span class="n">keep_prob</span>       
</code></pre></div>    </div>
  </li>
  <li>Vector d[l] is used for forward and back propagation and is the same for them, but it is different for each iteration (pass) or training example.</li>
  <li>At test time we don’t use dropout. If you implement dropout at test time - it would add noise to predictions.</li>
</ul>

<h3 id="understanding-dropout">Understanding Dropout</h3>

<ul>
  <li>In the previous video, the intuition was that dropout randomly knocks out units in your network. So it’s as if on every iteration you’re working with a smaller NN, and so using a smaller NN seems like it should have a regularizing effect.</li>
  <li>Another intuition: can’t rely on any one feature, so have to spread out weights.</li>
  <li>It’s possible to show that dropout has a similar effect to L2 regularization.</li>
  <li>Dropout can have different <code class="language-plaintext highlighter-rouge">keep_prob</code> per layer.</li>
  <li>The input layer dropout has to be near 1 (or 1 - no dropout) because you don’t want to eliminate a lot of features.</li>
  <li>If you’re more worried about some layers overfitting than others, you can set a lower <code class="language-plaintext highlighter-rouge">keep_prob</code> for some layers than others. The downside is, this gives you even more hyperparameters to search for using cross-validation. One other alternative might be to have some layers where you apply dropout and some layers where you don’t apply dropout and then just have one hyperparameter, which is a <code class="language-plaintext highlighter-rouge">keep_prob</code> for the layers for which you do apply dropouts.</li>
  <li>A lot of researchers are using dropout with Computer Vision (CV) because they have a very big input size and almost never have enough data, so overfitting is the usual problem. And dropout is a regularization technique to prevent overfitting.</li>
  <li>A downside of dropout is that the cost function J is not well defined and it will be hard to debug (plot J by iteration).
    <ul>
      <li>To solve that you’ll need to turn off dropout, set all the <code class="language-plaintext highlighter-rouge">keep_prob</code>s to 1, and then run the code and check that it monotonically decreases J and then turn on the dropouts again.</li>
    </ul>
  </li>
</ul>

<h3 id="other-regularization-methods">Other regularization methods</h3>

<ul>
  <li><strong>Data augmentation</strong>:
    <ul>
      <li>For example in a computer vision data:
        <ul>
          <li>You can flip all your pictures horizontally this will give you m more data instances.</li>
          <li>You could also apply a random position and rotation to an image to get more data.</li>
        </ul>
      </li>
      <li>For example in OCR, you can impose random rotations and distortions to digits/letters.</li>
      <li>New data obtained using this technique isn’t as good as the real independent data, but still can be used as a regularization technique.</li>
    </ul>
  </li>
  <li><strong>Early stopping</strong>:
    <ul>
      <li>In this technique we plot the training set and the dev set cost together for each iteration. At some iteration the dev set cost will stop decreasing and will start increasing.</li>
      <li>We will pick the point at which the training set error and dev set error are best (lowest training cost with lowest dev cost).</li>
      <li>We will take these parameters as the best parameters.
<img src="/coursera-dl/assets/improving-deep-neural-networks/02-_Early_stopping.png" alt="" /></li>
      <li>Andrew prefers to use L2 regularization instead of early stopping because this technique simultaneously tries to minimize the cost function and not to overfit which contradicts the orthogonalization approach (will be discussed further).</li>
      <li>But its advantage is that you don’t need to search a hyperparameter like in other regularization approaches (like <code class="language-plaintext highlighter-rouge">lambda</code> in L2 regularization).</li>
    </ul>
  </li>
  <li><strong>Model Ensembles</strong>:
    <ul>
      <li>Algorithm:
        <ul>
          <li>Train multiple independent models.</li>
          <li>At test time average their results.</li>
        </ul>
      </li>
      <li>It can get you extra 2% performance.</li>
      <li>It reduces the generalization error.</li>
      <li>You can use some snapshots of your NN at the training ensembles them and take the results.</li>
    </ul>
  </li>
</ul>

<h3 id="normalizing-inputs">Normalizing inputs</h3>

<ul>
  <li>If you normalize your inputs this will speed up the training process a lot.</li>
  <li>Normalization are going on these steps:
    <ol>
      <li>Get the mean of the training set: <code class="language-plaintext highlighter-rouge">mean = (1/m) * sum(x(i))</code></li>
      <li>Subtract the mean from each input: <code class="language-plaintext highlighter-rouge">X = X - mean</code>
        <ul>
          <li>This makes your inputs centered around 0.</li>
        </ul>
      </li>
      <li>Get the variance of the training set: <code class="language-plaintext highlighter-rouge">variance = (1/m) * sum(x(i)^2)</code></li>
      <li>Normalize the variance. <code class="language-plaintext highlighter-rouge">X /= variance</code></li>
    </ol>
  </li>
  <li>These steps should be applied to training, dev, and testing sets (but using mean and variance of the train set).</li>
  <li>Why normalize?
    <ul>
      <li>If we don’t normalize the inputs our cost function will be deep and its shape will be inconsistent (elongated) then optimizing it will take a long time.</li>
      <li>But if we normalize it the opposite will occur. The shape of the cost function will be consistent (look more symmetric like circle in 2D example) and we can use a larger learning rate alpha - the optimization will be faster.</li>
    </ul>
  </li>
</ul>

<h3 id="vanishing--exploding-gradients">Vanishing / Exploding gradients</h3>

<ul>
  <li>The Vanishing / Exploding gradients occurs when your derivatives become very small or very big.</li>
  <li>To understand the problem, suppose that we have a deep neural network with number of layers L, and all the activation functions are <strong>linear</strong> and each <code class="language-plaintext highlighter-rouge">b = 0</code>
    <ul>
      <li>Then:
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Y' = W[L]W[L-1].....W[2]W[1]X
</code></pre></div>        </div>
      </li>
      <li>
        <p>Then, if we have 2 hidden units per layer and x1 = x2 = 1, we result in:</p>

        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>if W[l] = [1.5   0] 
          [0   1.5] (l != L because of different dimensions in the output layer)
Y' = W[L] [1.5  0]^(L-1) X = 1.5^L 	# which will be very large
          [0  1.5]
</code></pre></div>        </div>
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>if W[l] = [0.5  0]
          [0  0.5]
Y' = W[L] [0.5  0]^(L-1) X = 0.5^L 	# which will be very small
          [0  0.5]
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>The last example explains that the activations (and similarly derivatives) will be decreased/increased exponentially as a function of number of layers.</li>
  <li>So If W &gt; I (Identity matrix) the activation and gradients will explode.</li>
  <li>And If W &lt; I (Identity matrix) the activation and gradients will vanish.</li>
  <li>Recently Microsoft trained 152 layers (ResNet)! which is a really big number. With such a deep neural network, if your activations or gradients increase or decrease exponentially as a function of L, then these values could get really big or really small. And this makes training difficult, especially if your gradients are exponentially smaller than L, then gradient descent will take tiny little steps. It will take a long time for gradient descent to learn anything.</li>
  <li>There is a partial solution that doesn’t completely solve this problem but it helps a lot - careful choice of how you initialize the weights (next video).</li>
</ul>

<h3 id="weight-initialization-for-deep-networks">Weight Initialization for Deep Networks</h3>

<ul>
  <li>A partial solution to the Vanishing / Exploding gradients in NN is better or more careful choice of the random initialization of weights</li>
  <li>In a single neuron (Perceptron model): <code class="language-plaintext highlighter-rouge">Z = w1x1 + w2x2 + ... + wnxn</code>
    <ul>
      <li>So if <code class="language-plaintext highlighter-rouge">n_x</code> is large we want <code class="language-plaintext highlighter-rouge">W</code>’s to be smaller to not explode the cost.</li>
    </ul>
  </li>
  <li>So it turns out that we need the variance which equals <code class="language-plaintext highlighter-rouge">1/n_x</code> to be the range of <code class="language-plaintext highlighter-rouge">W</code>’s</li>
  <li>So lets say when we initialize <code class="language-plaintext highlighter-rouge">W</code>’s like this (better to use with <code class="language-plaintext highlighter-rouge">tanh</code> activation):
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>np.random.rand(shape) * np.sqrt(1/n[l-1])
</code></pre></div>    </div>
    <p>or variation of this (Bengio et al.):</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>np.random.rand(shape) * np.sqrt(2/(n[l-1] + n[l]))
</code></pre></div>    </div>
  </li>
  <li>Setting initialization part inside sqrt to <code class="language-plaintext highlighter-rouge">2/n[l-1]</code> for <code class="language-plaintext highlighter-rouge">ReLU</code> is better:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>np.random.rand(shape) * np.sqrt(2/n[l-1])
</code></pre></div>    </div>
  </li>
  <li>Number 1 or 2 in the neumerator can also be a hyperparameter to tune (but not the first to start with)</li>
  <li>This is one of the best way of partially solution to Vanishing / Exploding gradients (ReLU + Weight Initialization with variance) which will help gradients not to vanish/explode too quickly</li>
  <li>The initialization in this video is called “He Initialization / Xavier Initialization” and has been published in 2015 paper.</li>
</ul>

<h3 id="numerical-approximation-of-gradients">Numerical approximation of gradients</h3>

<ul>
  <li>There is an technique called gradient checking which tells you if your implementation of backpropagation is correct.</li>
  <li>There’s a numerical way to calculate the derivative: <br />
<img src="/coursera-dl/assets/improving-deep-neural-networks/03-_Numerical_approximation_of_gradients.png" alt="" /></li>
  <li>Gradient checking approximates the gradients and is very helpful for finding the errors in your backpropagation implementation but it’s slower than gradient descent (so use only for debugging).</li>
  <li>Implementation of this is very simple.</li>
  <li>Gradient checking:
    <ul>
      <li>First take <code class="language-plaintext highlighter-rouge">W[1],b[1],...,W[L],b[L]</code> and reshape into one big vector (<code class="language-plaintext highlighter-rouge">theta</code>)</li>
      <li>The cost function will be <code class="language-plaintext highlighter-rouge">J(theta)</code></li>
      <li>Then take <code class="language-plaintext highlighter-rouge">dW[1],db[1],...,dW[L],db[L]</code> into one big vector (<code class="language-plaintext highlighter-rouge">d_theta</code>)</li>
      <li><strong>Algorithm</strong>:
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>eps = 10^-7   # small number
for i in len(theta):
  d_theta_approx[i] = (J(theta1,...,theta[i] + eps) -  J(theta1,...,theta[i] - eps)) / 2*eps
</code></pre></div>        </div>
      </li>
      <li>Finally we evaluate this formula <code class="language-plaintext highlighter-rouge">(||d_theta_approx - d_theta||) / (||d_theta_approx||+||d_theta||)</code> (<code class="language-plaintext highlighter-rouge">||</code> - Euclidean vector norm) and check (with eps = 10^-7):
        <ul>
          <li>if it is &lt; 10^-7  - great, very likely the backpropagation implementation is correct</li>
          <li>if around 10^-5   - can be OK, but need to inspect if there are no particularly big values in <code class="language-plaintext highlighter-rouge">d_theta_approx - d_theta</code> vector</li>
          <li>if it is &gt;= 10^-3 - bad, probably there is a bug in backpropagation implementation</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="gradient-checking-implementation-notes">Gradient checking implementation notes</h3>

<ul>
  <li>Don’t use the gradient checking algorithm at training time because it’s very slow.</li>
  <li>Use gradient checking only for debugging.</li>
  <li>If algorithm fails grad check, look at components to try to identify the bug.</li>
  <li>Don’t forget to add <code class="language-plaintext highlighter-rouge">lamda/(2m) * sum(W[l])</code> to <code class="language-plaintext highlighter-rouge">J</code> if you are using L1 or L2 regularization.</li>
  <li>Gradient checking doesn’t work with dropout because J is not consistent.
    <ul>
      <li>You can first turn off dropout (set <code class="language-plaintext highlighter-rouge">keep_prob = 1.0</code>), run gradient checking and then turn on dropout again.</li>
    </ul>
  </li>
  <li>Run gradient checking at random initialization and train the network for a while maybe there’s a bug which can be seen when w’s and b’s become larger (further from 0) and can’t be seen on the first iteration (when w’s and b’s are very small).</li>
</ul>

<h3 id="initialization-summary">Initialization summary</h3>

<ul>
  <li>
    <p>The weights W<sup>[l]</sup> should be initialized randomly to break symmetry</p>
  </li>
  <li>
    <p>It is however okay to initialize the biases b<sup>[l]</sup> to zeros. Symmetry is still broken so long as W<sup>[l]</sup> is initialized randomly</p>
  </li>
  <li>
    <p>Different initializations lead to different results</p>
  </li>
  <li>
    <p>Random initialization is used to break symmetry and make sure different hidden units can learn different things</p>
  </li>
  <li>
    <p>Don’t intialize to values that are too large</p>
  </li>
  <li>
    <p>He initialization works well for networks with ReLU activations.</p>
  </li>
</ul>

<h3 id="regularization-summary">Regularization summary</h3>

<h4 id="1-l2-regularization">1. L2 Regularization</h4>
<p><strong>Observations</strong>:</p>
<ul>
  <li>The value of λ is a hyperparameter that you can tune using a dev set.</li>
  <li>L2 regularization makes your decision boundary smoother. If λ is too large, it is also possible to “oversmooth”, resulting in a model with high bias.</li>
</ul>

<p><strong>What is L2-regularization actually doing?</strong>:</p>
<ul>
  <li>L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes.</li>
</ul>

<p><strong>What you should remember:</strong> <br />
Implications of L2-regularization on:</p>
<ul>
  <li>cost computation:
    <ul>
      <li>A regularization term is added to the cost</li>
    </ul>
  </li>
  <li>backpropagation function:
    <ul>
      <li>There are extra terms in the gradients with respect to weight matrices</li>
    </ul>
  </li>
  <li>weights:
    <ul>
      <li>weights end up smaller (“weight decay”) - are pushed to smaller values.</li>
    </ul>
  </li>
</ul>

<h4 id="2-dropout">2. Dropout</h4>
<p><strong>What you should remember about dropout:</strong></p>
<ul>
  <li>Dropout is a regularization technique.</li>
  <li>You only use dropout during training. Don’t use dropout (randomly eliminate nodes) during test time.</li>
  <li>Apply dropout both during forward and backward propagation.</li>
  <li>During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. For example, if <code class="language-plaintext highlighter-rouge">keep_prob</code> is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. You can check that this works even when keep_prob is other values than 0.5.</li>
</ul>

<h2 id="optimization-algorithms">Optimization algorithms</h2>

<h3 id="mini-batch-gradient-descent">Mini-batch gradient descent</h3>

<ul>
  <li>Training NN with a large data is slow. So to find an optimization algorithm that runs faster is a good idea.</li>
  <li>Suppose we have <code class="language-plaintext highlighter-rouge">m = 50 million</code>. To train this data it will take a huge processing time for one step.
    <ul>
      <li>because 50 million won’t fit in the memory at once we need other processing to make such a thing.</li>
    </ul>
  </li>
  <li>It turns out you can make a faster algorithm to make gradient descent process some of your items even before you finish the 50 million items.</li>
  <li>Suppose we have split m to <strong>mini batches</strong> of size 1000.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">X{1} = 0    ...  1000</code></li>
      <li><code class="language-plaintext highlighter-rouge">X{2} = 1001 ...  2000</code></li>
      <li><code class="language-plaintext highlighter-rouge">...</code></li>
      <li><code class="language-plaintext highlighter-rouge">X{bs} = ...</code></li>
    </ul>
  </li>
  <li>We similarly split <code class="language-plaintext highlighter-rouge">X</code> &amp; <code class="language-plaintext highlighter-rouge">Y</code>.</li>
  <li>So the definition of mini batches ==&gt; <code class="language-plaintext highlighter-rouge">t: X{t}, Y{t}</code></li>
  <li>In <strong>Batch gradient descent</strong> we run the gradient descent on the whole dataset.</li>
  <li>While in <strong>Mini-Batch gradient descent</strong> we run the gradient descent on the mini datasets.</li>
  <li>Mini-Batch algorithm pseudo code:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for t = 1:No_of_batches                         # this is called an epoch
	AL, caches = forward_prop(X{t}, Y{t})
	cost = compute_cost(AL, Y{t})
	grads = backward_prop(AL, caches)
	update_parameters(grads)
</code></pre></div>    </div>
  </li>
  <li>The code inside an epoch should be vectorized.</li>
  <li>Mini-batch gradient descent works much faster in the large datasets.</li>
</ul>

<h3 id="understanding-mini-batch-gradient-descent">Understanding mini-batch gradient descent</h3>

<ul>
  <li>In mini-batch algorithm, the cost won’t go down with each step as it does in batch algorithm. It could contain some ups and downs but generally it has to go down (unlike the batch gradient descent where cost function descreases on each iteration).
<img src="/coursera-dl/assets/improving-deep-neural-networks/04-_batch_vs_mini_batch_cost.png" alt="" /></li>
  <li>Mini-batch size:
    <ul>
      <li>(<code class="language-plaintext highlighter-rouge">mini batch size = m</code>)  ==&gt;    Batch gradient descent</li>
      <li>(<code class="language-plaintext highlighter-rouge">mini batch size = 1</code>)  ==&gt;    Stochastic gradient descent (SGD)</li>
      <li>(<code class="language-plaintext highlighter-rouge">mini batch size = between 1 and m</code>) ==&gt;    Mini-batch gradient descent</li>
    </ul>
  </li>
  <li>Batch gradient descent:
    <ul>
      <li>too long per iteration (epoch)</li>
    </ul>
  </li>
  <li>Stochastic gradient descent:
    <ul>
      <li>too noisy regarding cost minimization (can be reduced by using smaller learning rate)</li>
      <li>won’t ever converge (reach the minimum cost)</li>
      <li>lose speedup from vectorization</li>
    </ul>
  </li>
  <li>Mini-batch gradient descent:
    <ol>
      <li>faster learning:
        <ul>
          <li>you have the vectorization advantage</li>
          <li>make progress without waiting to process the entire training set</li>
        </ul>
      </li>
      <li>doesn’t always exactly converge (oscelates in a very small region, but you can reduce learning rate)</li>
    </ol>
  </li>
  <li>Guidelines for choosing mini-batch size:
    <ol>
      <li>If small training set (&lt; 2000 examples) - use batch gradient descent.</li>
      <li>It has to be a power of 2 (because of the way computer memory is layed out and accessed, sometimes your code runs faster if your mini-batch size is a power of 2):
  <code class="language-plaintext highlighter-rouge">64, 128, 256, 512, 1024, ...</code></li>
      <li>Make sure that mini-batch fits in CPU/GPU memory.</li>
    </ol>
  </li>
  <li>Mini-batch size is a <code class="language-plaintext highlighter-rouge">hyperparameter</code>.</li>
</ul>

<h3 id="exponentially-weighted-averages">Exponentially weighted averages</h3>

<ul>
  <li>There are optimization algorithms that are better than <strong>gradient descent</strong>, but you should first learn about Exponentially weighted averages.</li>
  <li>If we have data like the temperature of day through the year it could be like this:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>t(1) = 40
t(2) = 49
t(3) = 45
...
t(180) = 60
...
</code></pre></div>    </div>
  </li>
  <li>This data is small in winter and big in summer. If we plot this data we will find it some noisy.</li>
  <li>Now lets compute the Exponentially weighted averages:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>V0 = 0
V1 = 0.9 * V0 + 0.1 * t(1) = 4		# 0.9 and 0.1 are hyperparameters
V2 = 0.9 * V1 + 0.1 * t(2) = 8.5
V3 = 0.9 * V2 + 0.1 * t(3) = 12.15
...
</code></pre></div>    </div>
  </li>
  <li>General equation
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>V(t) = beta * v(t-1) + (1-beta) * theta(t)
</code></pre></div>    </div>
  </li>
  <li>If we plot this it will represent averages over <code class="language-plaintext highlighter-rouge">~ (1 / (1 - beta))</code> entries:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">beta = 0.9</code> will average last 10 entries</li>
      <li><code class="language-plaintext highlighter-rouge">beta = 0.98</code> will average last 50 entries</li>
      <li><code class="language-plaintext highlighter-rouge">beta = 0.5</code> will average last 2 entries</li>
    </ul>
  </li>
  <li>Best beta average for our case is between 0.9 and 0.98</li>
  <li><strong>Intuition</strong>: The reason why exponentially weighted averages are useful for further optimizing gradient descent algorithm is that it can give different weights to recent data points (<code class="language-plaintext highlighter-rouge">theta</code>) based on value of <code class="language-plaintext highlighter-rouge">beta</code>. If <code class="language-plaintext highlighter-rouge">beta</code> is high (around 0.9), it smoothens out the averages of skewed data points (oscillations w.r.t. Gradient descent terminology). So this reduces oscillations in gradient descent and hence makes faster and smoother path towerds minima.</li>
  <li>Another imagery example: <br />
  <img src="/coursera-dl/assets/improving-deep-neural-networks/Nasdaq1_small.png" alt="" /> <br />
  <em>(taken from <a href="https://www.investopedia.com/">investopedia.com</a>)</em></li>
</ul>

<h3 id="understanding-exponentially-weighted-averages">Understanding exponentially weighted averages</h3>

<ul>
  <li>Intuitions: <br />
  <img src="/coursera-dl/assets/improving-deep-neural-networks/05-_exponentially_weighted_averages_intuitions.png" alt="" /></li>
  <li>We can implement this algorithm with more accurate results using a moving window. But the code is more efficient and faster using the exponentially weighted averages algorithm.</li>
  <li>Algorithm is very simple:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>v = 0
Repeat
{
	Get theta(t)
	v = beta * v + (1-beta) * theta(t)
}
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="bias-correction-in-exponentially-weighted-averages">Bias correction in exponentially weighted averages</h3>

<ul>
  <li>The bias correction helps make the exponentially weighted averages more accurate.</li>
  <li>Because <code class="language-plaintext highlighter-rouge">v(0) = 0</code>, the bias of the weighted averages is shifted and the accuracy suffers at the start.</li>
  <li>To solve the bias issue we have to use this equation:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>v(t) = (beta * v(t-1) + (1-beta) * theta(t)) / (1 - beta^t)
</code></pre></div>    </div>
  </li>
  <li>As t becomes larger the <code class="language-plaintext highlighter-rouge">(1 - beta^t)</code> becomes close to <code class="language-plaintext highlighter-rouge">1</code></li>
</ul>

<h3 id="gradient-descent-with-momentum">Gradient descent with momentum</h3>

<ul>
  <li>The momentum algorithm almost always works faster than standard gradient descent.</li>
  <li>The simple idea is to calculate the exponentially weighted averages for your gradients and then update your weights with the new values.</li>
  <li>Pseudo code:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vdW = 0, vdb = 0
on iteration t:
	# can be mini-batch or batch gradient descent
	compute dw, db on current mini-batch                
  			
	vdW = beta * vdW + (1 - beta) * dW
	vdb = beta * vdb + (1 - beta) * db
	W = W - learning_rate * vdW
	b = b - learning_rate * vdb
</code></pre></div>    </div>
  </li>
  <li>Momentum helps the cost function to go to the minimum point in a more fast and consistent way.</li>
  <li><code class="language-plaintext highlighter-rouge">beta</code> is another <code class="language-plaintext highlighter-rouge">hyperparameter</code>. <code class="language-plaintext highlighter-rouge">beta = 0.9</code> is very common and works very well in most cases.</li>
  <li>In practice people don’t bother implementing <strong>bias correction</strong>.</li>
</ul>

<h3 id="rmsprop">RMSprop</h3>

<ul>
  <li>Stands for <strong>Root mean square prop</strong>.</li>
  <li>This algorithm speeds up the gradient descent.</li>
  <li>Pseudo code:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sdW = 0, sdb = 0
on iteration t:
	# can be mini-batch or batch gradient descent
	compute dw, db on current mini-batch
  	
	sdW = (beta * sdW) + (1 - beta) * dW^2  # squaring is element-wise
	sdb = (beta * sdb) + (1 - beta) * db^2  # squaring is element-wise
	W = W - learning_rate * dW / sqrt(sdW)
	b = B - learning_rate * db / sqrt(sdb)
</code></pre></div>    </div>
  </li>
  <li>RMSprop will make the cost function move slower on the vertical direction and faster on the horizontal direction in the following example:
  <img src="/coursera-dl/assets/improving-deep-neural-networks/06-_RMSprop.png" alt="" /></li>
  <li>Ensure that <code class="language-plaintext highlighter-rouge">sdW</code> is not zero by adding a small value <code class="language-plaintext highlighter-rouge">epsilon</code> (e.g. <code class="language-plaintext highlighter-rouge">epsilon = 10^-8</code>) to it: <br />
 <code class="language-plaintext highlighter-rouge">W = W - learning_rate * dW / (sqrt(sdW) + epsilon)</code></li>
  <li>With RMSprop you can increase your learning rate.</li>
  <li>Developed by Geoffrey Hinton and firstly introduced on <a href="https://www.coursera.org/">Coursera.org</a> course.</li>
</ul>

<h3 id="adam-optimization-algorithm">Adam optimization algorithm</h3>

<ul>
  <li>Stands for <strong>Adaptive Moment Estimation</strong>.</li>
  <li>Adam optimization and RMSprop are among the optimization algorithms that worked very well with a lot of NN architectures.</li>
  <li>Adam optimization simply puts RMSprop and momentum together!</li>
  <li>Pseudo code:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vdW = 0, vdW = 0
sdW = 0, sdb = 0
on iteration t:
	# can be mini-batch or batch gradient descent
	compute dw, db on current mini-batch                
  			
	vdW = (beta1 * vdW) + (1 - beta1) * dW     # momentum
	vdb = (beta1 * vdb) + (1 - beta1) * db     # momentum
  			
	sdW = (beta2 * sdW) + (1 - beta2) * dW^2   # RMSprop
	sdb = (beta2 * sdb) + (1 - beta2) * db^2   # RMSprop
  			
	vdW = vdW / (1 - beta1^t)      # fixing bias
	vdb = vdb / (1 - beta1^t)      # fixing bias
  			
	sdW = sdW / (1 - beta2^t)      # fixing bias
	sdb = sdb / (1 - beta2^t)      # fixing bias
  					
	W = W - learning_rate * vdW / (sqrt(sdW) + epsilon)
	b = B - learning_rate * vdb / (sqrt(sdb) + epsilon)
</code></pre></div>    </div>
  </li>
  <li>Hyperparameters for Adam:
    <ul>
      <li>Learning rate: needed to be tuned.</li>
      <li><code class="language-plaintext highlighter-rouge">beta1</code>: parameter of the momentum - <code class="language-plaintext highlighter-rouge">0.9</code> is recommended by default.</li>
      <li><code class="language-plaintext highlighter-rouge">beta2</code>: parameter of the RMSprop - <code class="language-plaintext highlighter-rouge">0.999</code> is recommended by default.</li>
      <li><code class="language-plaintext highlighter-rouge">epsilon</code>: <code class="language-plaintext highlighter-rouge">10^-8</code> is recommended by default.</li>
    </ul>
  </li>
</ul>

<h3 id="learning-rate-decay">Learning rate decay</h3>

<ul>
  <li>Slowly reduce learning rate.</li>
  <li>As mentioned before mini-batch gradient descent won’t reach the optimum point (converge). But by making the learning rate decay with iterations it will be much closer to it because the steps (and possible oscillations) near the optimum are smaller.</li>
  <li>One technique equations is<code class="language-plaintext highlighter-rouge">learning_rate = (1 / (1 + decay_rate * epoch_num)) * learning_rate_0</code>
    <ul>
      <li><code class="language-plaintext highlighter-rouge">epoch_num</code> is over all data (not a single mini-batch).</li>
    </ul>
  </li>
  <li>Other learning rate decay methods (continuous):
    <ul>
      <li><code class="language-plaintext highlighter-rouge">learning_rate = (0.95 ^ epoch_num) * learning_rate_0</code></li>
      <li><code class="language-plaintext highlighter-rouge">learning_rate = (k / sqrt(epoch_num)) * learning_rate_0</code></li>
    </ul>
  </li>
  <li>Some people perform learning rate decay discretely - repeatedly decrease after some number of epochs.</li>
  <li>Some people are making changes to the learning rate manually.</li>
  <li><code class="language-plaintext highlighter-rouge">decay_rate</code> is another <code class="language-plaintext highlighter-rouge">hyperparameter</code>.</li>
  <li>For Andrew Ng, learning rate decay has less priority.</li>
</ul>

<h3 id="the-problem-of-local-optima">The problem of local optima</h3>

<ul>
  <li>The normal local optima is not likely to appear in a deep neural network because data is usually high dimensional. For point to be a local optima it has to be a local optima for each of the dimensions which is highly unlikely.</li>
  <li>It’s unlikely to get stuck in a bad local optima in high dimensions, it is much more likely to get to the saddle point rather to the local optima, which is not a problem.</li>
  <li>Plateaus can make learning slow:
    <ul>
      <li>Plateau is a region where the derivative is close to zero for a long time.</li>
      <li>This is where algorithms like momentum, RMSprop or Adam can help.</li>
    </ul>
  </li>
</ul>

<h2 id="hyperparameter-tuning-batch-normalization-and-programming-frameworks">Hyperparameter tuning, Batch Normalization and Programming Frameworks</h2>

<h3 id="tuning-process">Tuning process</h3>

<ul>
  <li>We need to tune our hyperparameters to get the best out of them.</li>
  <li>Hyperparameters importance are (as for Andrew Ng):
    <ol>
      <li>Learning rate.</li>
      <li>Momentum beta.</li>
      <li>Mini-batch size.</li>
      <li>No. of hidden units.</li>
      <li>No. of layers.</li>
      <li>Learning rate decay.</li>
      <li>Regularization lambda.</li>
      <li>Activation functions.</li>
      <li>Adam <code class="language-plaintext highlighter-rouge">beta1</code>, <code class="language-plaintext highlighter-rouge">beta2</code> &amp; <code class="language-plaintext highlighter-rouge">epsilon</code>.</li>
    </ol>
  </li>
  <li>Its hard to decide which hyperparameter is the most important in a problem. It depends a lot on your problem.</li>
  <li>One of the ways to tune is to sample a grid with <code class="language-plaintext highlighter-rouge">N</code> hyperparameter settings and then try all settings combinations on your problem.</li>
  <li>Try random values: don’t use a grid.</li>
  <li>You can use <code class="language-plaintext highlighter-rouge">Coarse to fine sampling scheme</code>:
    <ul>
      <li>When you find some hyperparameters values that give you a better performance - zoom into a smaller region around these values and sample more densely within this space.</li>
    </ul>
  </li>
  <li>These methods can be automated.</li>
</ul>

<h3 id="using-an-appropriate-scale-to-pick-hyperparameters">Using an appropriate scale to pick hyperparameters</h3>

<ul>
  <li>Let’s say you have a specific range for a hyperparameter from “a” to “b”. It’s better to search for the right ones using the logarithmic scale rather then in linear scale:
    <ul>
      <li>Calculate: <code class="language-plaintext highlighter-rouge">a_log = log(a)  # e.g. a = 0.0001 then a_log = -4</code></li>
      <li>Calculate: <code class="language-plaintext highlighter-rouge">b_log = log(b)  # e.g. b = 1  then b_log = 0</code></li>
      <li>Then:
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>r = (a_log - b_log) * np.random.rand() + b_log
# In the example the range would be from [-4, 0] because rand range [0,1)
result = 10^r
</code></pre></div>        </div>
        <p>It uniformly samples values in log scale from [a,b].</p>
      </li>
    </ul>
  </li>
  <li>If we want to use the last method on exploring on the “momentum beta”:
    <ul>
      <li>Beta best range is from 0.9 to 0.999.</li>
      <li>You should search for <code class="language-plaintext highlighter-rouge">1 - beta in range 0.001 to 0.1 (1 - 0.9 and 1 - 0.999)</code> and the use <code class="language-plaintext highlighter-rouge">a = 0.001</code> and <code class="language-plaintext highlighter-rouge">b = 0.1</code>. Then:
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a_log = -3
b_log = -1
r = (a_log - b_log) * np.random.rand() + b_log
beta = 1 - 10^r   # because 1 - beta = 10^r
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ul>

<h3 id="hyperparameters-tuning-in-practice-pandas-vs-caviar">Hyperparameters tuning in practice: Pandas vs. Caviar</h3>

<ul>
  <li>Intuitions about hyperparameter settings from one application area may or may not transfer to a different one.</li>
  <li>If you don’t have much computational resources you can use the “babysitting model”:
    <ul>
      <li>Day 0 you might initialize your parameter as random and then start training.</li>
      <li>Then you watch your learning curve gradually decrease over the day.</li>
      <li>And each day you nudge your parameters a little during training.</li>
      <li>Called panda approach.</li>
    </ul>
  </li>
  <li>If you have enough computational resources, you can run some models in parallel and at the end of the day(s) you check the results.
    <ul>
      <li>Called Caviar approach.</li>
    </ul>
  </li>
</ul>

<h3 id="normalizing-activations-in-a-network">Normalizing activations in a network</h3>

<ul>
  <li>In the rise of deep learning, one of the most important ideas has been an algorithm called <strong>batch normalization</strong>, created by two researchers, Sergey Ioffe and Christian Szegedy.</li>
  <li>Batch Normalization speeds up learning.</li>
  <li>Before we normalized input by subtracting the mean and dividing by variance. This helped a lot for the shape of the cost function and for reaching the minimum point faster.</li>
  <li>The question is: <em>for any hidden layer can we normalize <code class="language-plaintext highlighter-rouge">A[l]</code> to train <code class="language-plaintext highlighter-rouge">W[l+1]</code>, <code class="language-plaintext highlighter-rouge">b[l+1]</code> faster?</em> This is what batch normalization is about.</li>
  <li>There are some debates in the deep learning literature about whether you should normalize values before the activation function <code class="language-plaintext highlighter-rouge">Z[l]</code> or after applying the activation function <code class="language-plaintext highlighter-rouge">A[l]</code>. In practice, normalizing <code class="language-plaintext highlighter-rouge">Z[l]</code> is done much more often and that is what Andrew Ng presents.</li>
  <li>Algorithm:
    <ul>
      <li>Given <code class="language-plaintext highlighter-rouge">Z[l] = [z(1), ..., z(m)]</code>, i = 1 to m (for each input)</li>
      <li>Compute <code class="language-plaintext highlighter-rouge">mean = 1/m * sum(z[i])</code></li>
      <li>Compute <code class="language-plaintext highlighter-rouge">variance = 1/m * sum((z[i] - mean)^2)</code></li>
      <li>Then <code class="language-plaintext highlighter-rouge">Z_norm[i] = (z[i] - mean) / np.sqrt(variance + epsilon)</code> (add <code class="language-plaintext highlighter-rouge">epsilon</code> for numerical stability if variance = 0)
        <ul>
          <li>Forcing the inputs to a distribution with zero mean and variance of 1.</li>
        </ul>
      </li>
      <li>Then <code class="language-plaintext highlighter-rouge">Z_tilde[i] = gamma * Z_norm[i] + beta</code>
        <ul>
          <li>To make inputs belong to other distribution (with other mean and variance).</li>
          <li>gamma and beta are learnable parameters of the model.</li>
          <li>Making the NN learn the distribution of the outputs.</li>
          <li><em>Note:</em> if <code class="language-plaintext highlighter-rouge">gamma = sqrt(variance + epsilon)</code> and <code class="language-plaintext highlighter-rouge">beta = mean</code> then <code class="language-plaintext highlighter-rouge">Z_tilde[i] = z[i]</code></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="fitting-batch-normalization-into-a-neural-network">Fitting Batch Normalization into a neural network</h3>

<ul>
  <li>Using batch norm in 3 hidden layers NN:
  <img src="/coursera-dl/assets/improving-deep-neural-networks/bn.png" alt="" /></li>
  <li>Our NN parameters will be:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">W[1]</code>, <code class="language-plaintext highlighter-rouge">b[1]</code>, …, <code class="language-plaintext highlighter-rouge">W[L]</code>, <code class="language-plaintext highlighter-rouge">b[L]</code>, <code class="language-plaintext highlighter-rouge">beta[1]</code>, <code class="language-plaintext highlighter-rouge">gamma[1]</code>, …, <code class="language-plaintext highlighter-rouge">beta[L]</code>, <code class="language-plaintext highlighter-rouge">gamma[L]</code></li>
      <li><code class="language-plaintext highlighter-rouge">beta[1]</code>, <code class="language-plaintext highlighter-rouge">gamma[1]</code>, …, <code class="language-plaintext highlighter-rouge">beta[L]</code>, <code class="language-plaintext highlighter-rouge">gamma[L]</code> are updated using any optimization algorithms (like GD, RMSprop, Adam)</li>
    </ul>
  </li>
  <li>If you are using a deep learning framework, you won’t have to implement batch norm yourself:
    <ul>
      <li>Ex. in Tensorflow you can add this line: <code class="language-plaintext highlighter-rouge">tf.nn.batch-normalization()</code></li>
    </ul>
  </li>
  <li>Batch normalization is usually applied with mini-batches.</li>
  <li>If we are using batch normalization parameters <code class="language-plaintext highlighter-rouge">b[1]</code>, …, <code class="language-plaintext highlighter-rouge">b[L]</code> doesn’t count because they will be eliminated after mean subtraction step, so:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Z[l] = W[l]A[l-1] + b[l] =&gt; Z[l] = W[l]A[l-1]
Z_norm[l] = ...
Z_tilde[l] = gamma[l] * Z_norm[l] + beta[l]
</code></pre></div>    </div>
    <ul>
      <li>Taking the mean of a constant <code class="language-plaintext highlighter-rouge">b[l]</code> will eliminate the <code class="language-plaintext highlighter-rouge">b[l]</code></li>
    </ul>
  </li>
  <li>So if you are using batch normalization, you can remove b[l] or make it always zero.</li>
  <li>So the parameters will be <code class="language-plaintext highlighter-rouge">W[l]</code>, <code class="language-plaintext highlighter-rouge">beta[l]</code>, and <code class="language-plaintext highlighter-rouge">alpha[l]</code>.</li>
  <li>Shapes:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">Z[l]       - (n[l], m)</code></li>
      <li><code class="language-plaintext highlighter-rouge">beta[l]    - (n[l], m)</code></li>
      <li><code class="language-plaintext highlighter-rouge">gamma[l]   - (n[l], m)</code></li>
    </ul>
  </li>
</ul>

<h3 id="why-does-batch-normalization-work">Why does Batch normalization work?</h3>

<ul>
  <li>The first reason is the same reason as why we normalize X.</li>
  <li>The second reason is that batch normalization reduces the problem of input values changing (shifting).</li>
  <li>Batch normalization does some regularization:
    <ul>
      <li>Each mini batch is scaled by the mean/variance computed of that mini-batch.</li>
      <li>This adds some noise to the values <code class="language-plaintext highlighter-rouge">Z[l]</code> within that mini batch. So similar to dropout it adds some noise to each hidden layer’s activations.</li>
      <li>This has a slight regularization effect.</li>
      <li>Using bigger size of the mini-batch you are reducing noise and therefore regularization effect.</li>
      <li>Don’t rely on batch normalization as a regularization. It’s intended for normalization of hidden units, activations and therefore speeding up learning. For regularization use other regularization techniques (L2 or dropout).</li>
    </ul>
  </li>
</ul>

<h3 id="batch-normalization-at-test-time">Batch normalization at test time</h3>

<ul>
  <li>When we train a NN with Batch normalization, we compute the mean and the variance of the mini-batch.</li>
  <li>In testing we might need to process examples one at a time. The mean and the variance of one example won’t make sense.</li>
  <li>We have to compute an estimated value of mean and variance to use it in testing time.</li>
  <li>We can use the weighted average across the mini-batches.</li>
  <li>We will use the estimated values of the mean and variance to test.</li>
  <li>This method is also sometimes called “Running average”.</li>
  <li>In practice most often you will use a deep learning framework and it will contain some default implementation of doing such a thing.</li>
</ul>

<h3 id="softmax-regression">Softmax Regression</h3>

<ul>
  <li>In every example we have used so far we were talking about binary classification.</li>
  <li>There are a generalization of logistic regression called Softmax regression that is used for multiclass classification/regression.</li>
  <li>For example if we are classifying by classes <code class="language-plaintext highlighter-rouge">dog</code>, <code class="language-plaintext highlighter-rouge">cat</code>, <code class="language-plaintext highlighter-rouge">baby chick</code> and <code class="language-plaintext highlighter-rouge">none of that</code>
    <ul>
      <li>Dog <code class="language-plaintext highlighter-rouge">class = 1</code></li>
      <li>Cat <code class="language-plaintext highlighter-rouge">class = 2</code></li>
      <li>Baby chick <code class="language-plaintext highlighter-rouge">class = 3</code></li>
      <li>None <code class="language-plaintext highlighter-rouge">class = 0</code></li>
      <li>To represent a dog vector <code class="language-plaintext highlighter-rouge">y = [0 1 0 0]</code></li>
      <li>To represent a cat vector <code class="language-plaintext highlighter-rouge">y = [0 0 1 0]</code></li>
      <li>To represent a baby chick vector <code class="language-plaintext highlighter-rouge">y = [0 0 0 1]</code></li>
      <li>To represent a none vector <code class="language-plaintext highlighter-rouge">y = [1 0 0 0]</code></li>
    </ul>
  </li>
  <li>Notations:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">C = no. of classes</code></li>
      <li>Range of classes is <code class="language-plaintext highlighter-rouge">(0, ..., C-1)</code></li>
      <li>In output layer <code class="language-plaintext highlighter-rouge">Ny = C</code></li>
    </ul>
  </li>
  <li>Each of C values in the output layer will contain a probability of the example to belong to each of the classes.</li>
  <li>In the last layer we will have to activate the Softmax activation function instead of the sigmoid activation.</li>
  <li>Softmax activation equations:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>t = e^(Z[L])                      # shape(C, m)
A[L] = e^(Z[L]) / sum(t)          # shape(C, m), sum(t) - sum of t's for each example (shape (1, m))
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="training-a-softmax-classifier">Training a Softmax classifier</h3>

<ul>
  <li>There’s an activation which is called hard max, which gets 1 for the maximum value and zeros for the others.
    <ul>
      <li>If you are using NumPy, its <code class="language-plaintext highlighter-rouge">np.max</code> over the vertical axis.</li>
    </ul>
  </li>
  <li>The Softmax name came from softening the values and not harding them like hard max.</li>
  <li>Softmax is a generalization of logistic activation function to <code class="language-plaintext highlighter-rouge">C</code> classes. If <code class="language-plaintext highlighter-rouge">C = 2</code> softmax reduces to logistic regression.</li>
  <li>The loss function used with softmax:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>L(y, y_hat) = - sum(y[j] * log(y_hat[j])) # j = 0 to C-1
</code></pre></div>    </div>
  </li>
  <li>The cost function used with softmax:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>J(w[1], b[1], ...) = - 1 / m * (sum(L(y[i], y_hat[i]))) # i = 0 to m
</code></pre></div>    </div>
  </li>
  <li>Back propagation with softmax:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dZ[L] = Y_hat - Y
</code></pre></div>    </div>
  </li>
  <li>The derivative of softmax is:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Y_hat * (1 - Y_hat)
</code></pre></div>    </div>
  </li>
  <li>Example:
  <img src="/coursera-dl/assets/improving-deep-neural-networks/07-_softmax.png" alt="" /></li>
</ul>

<h3 id="deep-learning-frameworks">Deep learning frameworks</h3>

<ul>
  <li>It’s not practical to implement everything from scratch. Our numpy implementations were to know how NN works.</li>
  <li>There are many good deep learning frameworks.</li>
  <li>Deep learning is now in the phase of doing something with the frameworks and not from scratch to keep on going.</li>
  <li>Here are some of the leading deep learning frameworks:
    <ul>
      <li>Caffe/ Caffe2</li>
      <li>CNTK</li>
      <li>DL4j</li>
      <li>Keras</li>
      <li>Lasagne</li>
      <li>mxnet</li>
      <li>PaddlePaddle</li>
      <li>TensorFlow</li>
      <li>Theano</li>
      <li>Torch/Pytorch</li>
    </ul>
  </li>
  <li>These frameworks are getting better month by month. Comparison between them can be found <a href="https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software">here</a>.</li>
  <li>How to choose deep learning framework:
    <ul>
      <li>Ease of programming (development and deployment)</li>
      <li>Running speed</li>
      <li>Truly open (open source with good governance)</li>
    </ul>
  </li>
  <li>Programming frameworks can not only shorten your coding time but sometimes also perform optimizations that speed up your code.</li>
</ul>

<h3 id="tensorflow">TensorFlow</h3>

<ul>
  <li>In this section we will learn the basic structure of TensorFlow programs.</li>
  <li>Lets see how to implement a minimization function:
    <ul>
      <li>Example function: <code class="language-plaintext highlighter-rouge">J(w) = w^2 - 10w + 25</code></li>
      <li>The result should be <code class="language-plaintext highlighter-rouge">w = 5</code> as the function is <code class="language-plaintext highlighter-rouge">(w-5)^2 = 0</code></li>
      <li>Code v.1:
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
    
    
<span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>                 <span class="c1"># creating a variable w
</span><span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">w</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">multiply</span><span class="p">(</span><span class="o">-</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">w</span><span class="p">)),</span> <span class="mf">25.0</span><span class="p">)</span>        <span class="c1"># can be written as this - cost = w**2 - 10*w + 25
</span><span class="n">train</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="mf">0.01</span><span class="p">).</span><span class="n">minimize</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">session</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
<span class="n">session</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>    <span class="c1"># Runs the definition of w, if you print this it will print zero
</span><span class="n">session</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"W after one iteration:"</span><span class="p">,</span> <span class="n">session</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
	<span class="n">session</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"W after 1000 iterations:"</span><span class="p">,</span> <span class="n">session</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>Code v.2 (we feed the inputs to the algorithm through coefficients):</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
    
    
<span class="n">coefficients</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">10.</span><span class="p">],</span> <span class="p">[</span><span class="mf">25.</span><span class="p">]])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>                 <span class="c1"># Creating a variable w
</span><span class="n">cost</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">w</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">w</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

<span class="n">train</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">GradientDescentOptimizer</span><span class="p">(</span><span class="mf">0.01</span><span class="p">).</span><span class="n">minimize</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>

<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">session</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
<span class="n">session</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>    <span class="c1"># Runs the definition of w, if you print this it will print zero
</span><span class="n">session</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">coefficients</span><span class="p">})</span>

<span class="k">print</span><span class="p">(</span><span class="s">"W after one iteration:"</span><span class="p">,</span> <span class="n">session</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
	<span class="n">session</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">coefficients</span><span class="p">})</span>

<span class="k">print</span><span class="p">(</span><span class="s">"W after 1000 iterations:"</span><span class="p">,</span> <span class="n">session</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>In TensorFlow you implement only the forward propagation and TensorFlow will do the backpropagation by itself.</li>
  <li>In TensorFlow a placeholder is a variable you can assign a value to later.</li>
  <li>If you are using a mini-batch training you should change the <code class="language-plaintext highlighter-rouge">feed_dict={x: coefficients}</code> to the current mini-batch data.</li>
  <li>Almost all TensorFlow programs use this:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">session</span><span class="p">:</span>       <span class="c1"># better for cleaning up in case of error/exception
</span>	<span class="n">session</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
	<span class="n">session</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>In deep learning frameworks there are a lot of things that you can do with one line of code like changing the optimizer.
<em><strong>Side notes:</strong></em></li>
  <li>Writing and running programs in TensorFlow has the following steps:
    <ol>
      <li>Create Tensors (variables) that are not yet executed/evaluated.</li>
      <li>Write operations between those Tensors.</li>
      <li>Initialize your Tensors.</li>
      <li>Create a Session.</li>
      <li>Run the Session. This will run the operations you’d written above.</li>
    </ol>
  </li>
  <li>Instead of needing to write code to compute the cost function we know, we can use this line in TensorFlow :
<code class="language-plaintext highlighter-rouge">tf.nn.sigmoid_cross_entropy_with_logits(logits = ...,  labels = ...)</code></li>
  <li>To initialize weights in NN using TensorFlow use:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>W1 = tf.get_variable("W1", [25,12288], initializer = tf.contrib.layers.xavier_initializer(seed = 1))

b1 = tf.get_variable("b1", [25,1], initializer = tf.zeros_initializer())
</code></pre></div>    </div>
  </li>
  <li>For 3-layer NN, it is important to note that the forward propagation stops at <code class="language-plaintext highlighter-rouge">Z3</code>. The reason is that in TensorFlow the last linear layer output is given as input to the function computing the loss. Therefore, you don’t need <code class="language-plaintext highlighter-rouge">A3</code>!</li>
  <li>To reset the graph use <code class="language-plaintext highlighter-rouge">tf.reset_default_graph()</code></li>
</ul>

<h2 id="extra-notes">Extra Notes</h2>

<ul>
  <li>If you want a good papers in deep learning look at the ICLR proceedings (Or NIPS proceedings) and that will give you a really good view of the field.</li>
  <li>Who is Yuanqing Lin?
    <ul>
      <li>Head of Baidu research.</li>
      <li>First one to win ImageNet</li>
      <li>Works in PaddlePaddle deep learning platform.</li>
    </ul>
  </li>
</ul>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">
   <div align="center" class="wrap">
      <div align="center" class="footer-col-1 column">
         <ul>
            <li>
               
               <span class="icon github">
                  <a href="https://github.com/amanchadha">
                     <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                        viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                        <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                           c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                           c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                           c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                           C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                           c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                           c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                           c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                           c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                     </svg>
                  </a>
               </span>
               <!-- <span class="username">amanchadha</span> -->
                | 
               <a href="https://citations.amanchadha.com/">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                     viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABJoAAAVjBAMAAABzrVjQAAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAElBMVEX///+xsLCxsLCxsLCx
                        sLD///+bxiTSAAAABHRSTlMAAKP3FWDuDwAAAAFiS0dEAIgFHUgAAAAJcEhZcwAACxMAAAsTAQCa
                        nBgAAAAHdElNRQfkBwQDMic2f+cwAAA03klEQVR42u2dW3IdOZJEu81mAcMqbOCacQMy0wImVNr/
                        msZKKpVeuHkzEA8PIPx8douAh+MkkmKR1H/+QwghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQ
                        QgghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIeQQ/vt2KOMzyeH/GtiE7rgP/3u+TQPdcRukgU3o
                        jtsgb+fbNNAlt+GtgU3ojtsgDWxCd9yGT2/n2zTQJbfhrYFN6I7bIA1sGuiS2/DWwCZ0x214a2DT
                        QJfcBelgE7rkNrw1sAndcRukgU0DXXIXvsl0tE3oktvwb+MH2zTQJXdBOtiELrkL32U62KaBbrkL
                        P3R+rE1/oEvugnSwCV1yF/76sfRTbRrolrvwU+un2oQuuQvSwaaBbrkLP9d+qE3okrvwS+1n2jTQ
                        LTdBOtj0J7rlLvxa/JE2oUvugnSwaaBbbsJvMh1pE7rlLvze/IE2DXTLTZAWNqFbbsKnSfXn2TTQ
                        NTdh1v1xNg10y02QFjahW+7CtPzTbBrolpswb/80m9AtN0Fa2DTQNTfhSf2H2YRuuQmPFja9o2vu
                        gTzr/yibBrrmJjw9gKNsQrfcBGlh00DX3IPnMh1lE7rmJlycwEE2DXTNPZAeNqFrbsLVEZxj00d0
                        zT24PINjbBromnsgPWxC19yE60M4xaaBrrkHL07hFJvQNfdAetg00D334NUxHGITuuYeSA+b0DU3
                        oYdNA11zE3rYhG65Cy1sGuiWu9DCJnTJbehg00CX3IYGNg10x31oYBO64kacb9NAV9yI821CN9yJ
                        420a6IY7cbxN6IJbcbpNA11wKw63ib8YPJXDbULX24yzbRroeptxtk3odrtxtE0D3W43jrYJXW47
                        TrbpHV1uOw62aaC77cfBNqGrbci5Ng10tQ051yZ0sx051qaBbrYjx9qELrYlp9qE7rUnh9o00L32
                        5FCb0LU25UybBrrWppxpE7rVrhxp00C32pUjbUKX2pYTbfqILrUtB9o00J325UCb0JW2YEz/1/Ns
                        GqpWyBIy5v/zcTahi27Bk2f2OJuGqhWyhDSx6Q900R2QtyY2oYtuwVsTm4aqFbKEdLEJXXQLnj+2
                        Z9k0VK2QJS6aPssmdNEdkC42DVUtZImrqk+yaahaIUtIF5vQRXfgr8sn9yCbhqoWssR12QfZhC66
                        A//qMq7/7+1tGqpayBIv2j7HJnTRHfhuy3jx/29u01DVQlaQV3WfYtNQ1UKWeNn3KTahi+7Aj66M
                        l39iY5uGqhayxOvCD7EJXXQHPnSxaahqISvIjcbPsAnddAfuPL9H2PSObroBv4gy7vyhLW0aqlrI
                        Erc6P8EmdNEdeHSxaahqISvIvdIPsAnddAduPsL72zRUtZAV5Gbr+9uEbroDd5/h7W1CF92BSe1j
                        +gd3t2moaiEryO3ed7cJ3XQH7j/Fm9s0VLWQFeR+8ZvbhG66AZ8Uj/HeNg1VL2QFTfN724RuugFP
                        BBmaP7yHTeimO/Ck+jH9wzvbNFS1kBVE1f3ONqGbbsBTPYbuj9e3aah6ISsoy9/YJnTTDXhux1D+
                        +eo2DVUvZAVt+9vaxF8MHs+FHEP7AbVtQjfdgCs3hvojKts0VL2QFfT972oTuukGXKox9B9S16ah
                        6oWssHAAm9qEbroBK4/znjYNVS9kAVk5gS1tGqpeyApLR7ClTeimG/DKi7H0URVtGqpeyAJ/rZ3B
                        jjahq27A4hO9oU1D1QtZ4KUVb3+ufVw9m9BVN8BwBJvZNNBVn49YTuDyg6vZNG71QQy8kOnlx29k
                        E7rqBpgPYBubBrrq8xHzATxfoZhN6Kob4HAAm9g00FWfj1wewLttjVo2oas+H3E5gC1sekd3fT7X
                        BzBurvLMyUo23Z2FLCPXJ3B7nQ1sQlfdAK8TqG/TQFd9Pg+vE5DyNqGr/p1fAn5E57EiL05gWFeq
                        Y9P9UVLYIqTLTN95v71SeZvQVd+tfaDDLSOOR1Dcpo/oru+XPtAJF3l5Bua1qthU5oReulQqrYbX
                        Y5kXq2ITuur7ldfKe58bj4m5pyI2DXTXVx1N+dO+W7XZhnm1Ijahq/6C6DJ/ROf1Hm6Yl6thk2aO
                        MPZM7TidZp7KNqGr/nzjR8yK5r7JnWmGYr3CNqGr/qx+yxVK7jfeMC9YwSbNFMi2N9bJ/Rzq2oSu
                        2iBThfBu8w3zigVs0gyBLHtfnW7ON8xLFrAJ3bVNpgL5X3JzkGHuDG+TZoYQPlknQA/wCrk5xzCv
                        CbcJ/4vB7TOgJ3AacCjWLGoTumoHmQo8EVfI3TGGeVG0TZoJQjhjigtuy3SATft0vatOMUOUtAl9
                        Ck4yFf5RQMWEw7ws2CZ01+dM4jDhUCxb0SZNfnDVm+oUdRoFbUJ/x5mYJ1g8i5oTaiYoaBO4avOX
                        LevrFDZAPZvQ9XvPAx5ngqjyD/PKSJu2qtr5NFJQfgegJn85m9DlnzeRcUJN/HI2gasW8wDlRjJO
                        OMxr42zSZI/gxJlsE2rSy3QFmE3o4h8hU4GH+gnRhh/mxWE27Vb1HmP9gP7rH8PcH8omTfIITp3L
                        MqEmu0xXQNkErlrMAxQdzDLhMC8PskkTPIJzJzNMqIku0xVANoGrFvMAZUczTDjM62NsQncdOZvm
                        TMKQ6OTzDSA2oQtf6vo24OG+EH4s8wohNm3Z9T7jrT4uw7wDwiZN6jpd7zPf6uOiyT3vEGETumsx
                        T1B8wMda7GHuEGCTJnQIp08oCbHnewBswladcDWhR1xNPcwl5tv0EVt1wtUEvpwkI/V8k3SboEWb
                        ytaAHDDlaOYtptuE7NlWdtTBlJlPE1qmK2TbBKzZ3LYG2HiynllzNvNtsm2CtezQdtTJuJKUeV5j
                        sk2wkj3aVgEaTwyRNYcz3yfXJvwvOrK0reIjZj5L5GHuMdcmTMOvWwhAczR+pEWe95hqE6Zhv7pV
                        IKYTU2LN8cx3SrUJUbBn3WFn40Ve4nmRmTYh+r1VQgzbTac5n/lWmTbl1/sb0SOuHo4Lkhh4vlei
                        Tent+vetI3u6zAOaN5lnkyZrFGKeovDE5uE0eeeb5dmUW+2c2Aktp1NhOE1ema6QZlNytYoK4kgd
                        7kPqEc2rTLMptVlVBXFojqfAbJq48+2ybMos9imRA07ZazbNGcl0hSybkr1RNBCJ5nzws2nSzvdL
                        simv1uDGlaTNln1I8y5zbNLkrN24ko87jaY5JZmukGNTvjkTlv6N+sQDMiDmoNqw8x1TbErqNKVy
                        JTmj5R/TvMwUmxDuRFUeeELLiDmmOut8ywybUhp9TdR41yQM5vUK15yTTFfIsAlkz635w/kYP5lX
                        1GFuM8Gmd4w99+YPR3NE4ME0UeebxtsUX+c9YqZ7zT6DaU5KpivE2wTTJ6r0wDNaQcwJV5LOdw23
                        KbrM/NK1xM7l+I/saY5qXme4TUiDfuQRMh2+AcegQ7GtTFeItkmTcJfWK1Ug5nhrQef7RtsENegH
                        nP/V1TIdeOYcin1lukKwTViFXk6fg+aUkGNpcs43jrUpsEclD//hCrQg5myrOec7x9qEdug7/rNV
                        qME35lDsLNMVQm3SxAvGX5ECPYg52XLM+dahNqEVCus98Jg0AGPOC420KarEBcQ8jImYoR7A85oX
                        GmkTWqEfcPcj7pyMB5qUcr55oE1og37E3Y8CXbiHHIrNZbpCnE2abNGIeRojW8ykObH57nE2oQ2K
                        bT7woG6CDTlvNMymgP4qNa9lh5E0RybTFcJsQgsUXT24DjEn+p1h3j/KJk2yeAKqB/ch5kS2jPP9
                        g2zC/2Lw6Oq1eM8UEHGYKw2yCe3PndFz8Z4pIOIwVxpjkyZXAhF2oCsRcyJTxPn2MTah9fmFCDvg
                        nfgnHIrdZbpCiE2aWBnE6KGk/lCaY5PpCiE2oe25NXk2mrPCTKVJON89wibv3ur1vsKf5afSnNt8
                        9wCbNKFS8O99Ce+x3ANqDm7eaYBNaHnia48/rPUDTQo439zfJu/S7MQJAi1GzIkMAeeb+9uEdud3
                        4gTBNuOdbyj2lukK7jZpIuUg5pmceHcezDuf5ujmpbrbhHbn7uAANKeFGEyTb763t03ejRUsfZ3i
                        g2nObr63s02aQFnEGqKh+GSaw5PpCs42oc1J6DzpuACTaeLJdAVfm7zrqth5oXrEnGg53nxrX5vQ
                        4kyJdgTXj5gD/cQwb+1qkyZOGs6Vm3h3ns03neb45q262oQWRzE3Bs153QGXbt6qp03vaHEUc4Mo
                        Pdsw7+xokyZMIr6NGyk9m+YA5zs72oTW5gkZkoQcWPpwmnAyXcHPJu+iShaeeWDpw2nCyXQFP5vQ
                        1qQUbsZ5ODEH+oFh3tjNJk2UVJI0uYnzcGIOtHiE843dbEJLk9J37ollT6fJNt/Yyya0NDl9557Y
                        HVDZ5rU62eRdkh9imCoC5/E8o2kOcV6rk01oZ7Rj4yg83jDv62OTJkcyiaIgqhJzoLVo8319bEIr
                        c0GiKO5HljyfJppMV3Cxybuhom37UHc+zTHKdAUXm9DGJLXtQ935hmJbma7gYRNamKy2889s+VjD
                        k823dbDJuZ6yZQPOLHdATbL5tg42oYXJKtuLsgMO87Z2mzQZ8nEs2wvnCf2CaU5y3qvdJrQvK1ND
                        0RzaDTDB5r2abXLuxhtZGioU58Ywwea9Wm2q9YvB706NpeqEw7yr1Sa0La8AyJLcmZjzfGOYdzXa
                        pAkAAWFLbmlizrOSa76r0Sa0LC9B2OJ5apkjanLJdAWbTc69BIDx5QVFR9ScpkxXsNmEdiWxak+K
                        jjgUm8p0BZNNmu1BgHy5xndEMedZOM75phabvH+fegCfUMK4HdvqwQbHmm9qsQmtSmbTrmiOLXFG
                        Taz5pgabfDuJwa1pX3yH9EqlOdB5sQab0KasDw3Hd0ivVMNc7LpNmr1hiG6mLHy7Q6SaF7tuE1oU
                        w9BwNOf2GkSqebHLNvkWEoWoZkrDtzyvITWp5nuu2uTbRxheRXtTckjNmc73XLUJrUly0d6UHHKY
                        91y0SbMxkgfUmaT+xBxHH2q+56JNaEvugnXG5+Be4vUFf00oma6wZpNrGZGgrckpEBBKpius2YSW
                        JLtnfypOORRbynSFJZve0ZJk9+xPxSmHYkuZrrBik2ZXMGhpnlJxSs25ynSFFZvQiuT37I/m5BaP
                        NjTTfMsFm1yLCAYtTU6JYo6jzjTfcsEmtCEa0NLktCjmNF8Y5i31Nmn2hIN25jmeU4o5jfpk51vq
                        bUILogLtTE6NYk7zhWHeUm3TR7QgKtDO+BzdK/7KjyTTFbQ2eZaQANqZpCLzI8l0Ba1NaD0gNUeg
                        ObqkMTWRZLqC0ibXDhJAO3NBvTE1hyvTFZQ2oe3A1BxCvTGHYkeZrqCzSbNfCdDKXFBvTM3pynQF
                        nU1oOUA1h6A5u7WzjUw031FlE9oNPWhlnM5u7WwjE8131NjkOX8SaGWS2hRzGm2i+Y4am9BqLIBW
                        5gLP3xgq5jR/M8w7KmzSbFYFtDJXOI4p5jDaA57vqLAJbcYKaGOS+hRzmL8Z5h3v26TZqwxoY65w
                        HFPMYbQnPN/xtk2areqANiar0fRAMl3htk1oL4AtB6E5vJQ5NYFkusJdmzxHTwRtTFal6YFkusJd
                        m9BaIFuOotqcQ7GhTFe4aZNmo0qghbmk2pyaQ5bpCjdtQlsBbTmKanMOxYYyXeGeTZp9SoEWxu30
                        MubU5JHpCrds2uAXg0e2HIXm9F7wyM4j0xVu2YR2Yh20MG6n9wIxh1HmmW94xybHqbNBC+N2ei8Q
                        cxhlnvmGd2xCK2EALcw1fnOKOctbkk2aTaqB9uUavznFnEV50PMNb9iENsIC2pdr/OYUc5a3HJve
                        0UZYQPvid3zXiDmLMs58w5c2+U2MAO2L3/FdI+YsyjjzDV/ahPbBBtoXv+O7RsxZlHHmG76yyW9g
                        CGhf/I7vGjFnUcaZb/jKJrQORtC+vMBtTjFHeUuwSbNBRdC6vKDWoEOxn0xXeGET2oYKJQdSa9Ch
                        2E+mK1zbhJbBjLyVxm9QjzTDXOylTZrlayJvpfErODvNvNhLm9Au2JG30mjO75rsNPNir2zymxWG
                        vJXGr+HsNPNir2xCq+CAvJVGc37XZKeZF3thk9+oOOStNm6DeoQZ5mIvbEKb4IG81cZtUI8ww1zs
                        c5vQIrggrxoE4zaoR5hhLvapTZql60KbatiE9sCH6jZpDvCS2ja5jYmFNpWwCa2BE7Spgk1uU6JB
                        60KbfH/DJxa0Lq+oNOdQ7KexCe2AH2hbaNM57znaVMAmtAKOoG2hTZpVq4O2hTahDfAEbYvnCUbP
                        qcly2yavAUuAtqW7TV7z1QBtS3eb0Ofvi9ytkjZF2OQ1XhFoE9Qm9PE7U90mr75r2qRZcQdoE9Im
                        9Ol7Q5uANmkW3ALahLNJs94e0CacTeiz96e8TZojvKCgTU6TlQJtS1+b0CcfAdqWtjY5DVYLtC1t
                        bUIffAhoW7ra9D/ogw8BbctL6ow5FPu9tOm/6F5DekYPkTMlbfIcTjt2IWhTCrSJNvlBm2iTH7SJ
                        NvnRxKZ32pRBE5t8xsxOMq/1eJvKf4mANm1Ucw+bPmUnkekKtAmNy5iSnWS+IW1CQ5v2qZk2xSSZ
                        b0ib4NCmDJxscik6kjJD0ibaRJvch6NNyYXPN6RNcMoMSZtoE21yH+4K9ByvoE0Z0KbkGWnTATZ5
                        zJkeRKYr0CY4tGmXlp8PXgfatEvLtCkoyLxU2gSHNu3SMm0KCjIvlTbBoU27tOxXdek5JT3IfEfa
                        BIc2bdLyFjY5fDFc0guf70ib8NCmBNxs8uk6jioT0ibaRJvch2tu0yO98HmntAmPfcL8wueddrCp
                        +qfhVQakTbSJNrkPR5uSC5fpCrQJT5UBaZNh9DJUmY820SbalNTyJja9F5mPNp1gk+YUadMqbjYV
                        /zScNmVAm3LHo020iTbdw8+mD+hRLqFNGfjZJOYskdCmDGhT7nS0iTbRpnv42VT703DalAFtok11
                        Wu5iEyKHTFdoYpOYwxSeE5FjXihtwkObdmiZNsXlmBfaxCaXf7+t6JyCyDHftIlNpT8Np00btEyb
                        AnPMN6VNeGhTCo42uVUegNEmSI55n7QJD21KgTbRJj8cbar8iZPNJoHkmO9Km/DQphRoE23y493R
                        Jr/S3bHZhMkxr7O0TbaaaVNgjnmdtAkPbdqg5rDWS40pmBzzbWkTHtpUv+YmNn3A5JDpCqVtcv0S
                        gZjTRGGyCZRj3iZtwkObcvC0qe6rzmKTgHLM96VNeGhTDrSJNtXoObT4MlOicszLpE14aFP9nlvY
                        5DoTbdKAniZgSkHlmG9Mm/DQpiRcbXKt3hGDTbAc8y5pEx7DSK45aFNvm3wnok0q0NO4DymuOY63
                        STNgQ5twZct0hVY2iTlPCLQpCdpEm/z409Wmoq+6Kk/H8Ta1+DScNpVvesoDPY7vjM45aJMOMecJ
                        QHOItAlU9fk2eU9Dm5Sgx3EdUYBB5nvTJjjLIyKDyHSF6jb94WuTmAP5Q5vyoE1ps9AmLehxJrzT
                        pjTOt6nMKA1s0oy43AIU2pQHbUqbhDapQc/zO7SpftnH2+QfhDapEXMgZzRnSJus0KasOTrYtNj2
                        U9DzOM33AZtEpivQJjSL84GTyHSF+jad/qp7LzMFbapxDvnjRUxBmxZAz+MyXkSSFjZphqRNSUXL
                        dIWGNok5kSt1ZqBNC3xCD+Qw3Qd0FJmusIFNZ3/itGYTPIpMV+hok5gTObJkU8wEPWxaKpw2hUaZ
                        J+hoU6lXXaEBaFOhw1hjJf9f+J5lusIONh39iVOh/LRpiUJfI9Cc4L8UyCLTFbawaalywHFkjVYg
                        i0xX6GmTmCMhR4tK38Smgz9xWrGpQpZ5gz1tqvOqqxS+i03v3jY90BN9YyG7RGXpYpNmTuyJaFnI
                        XqLleYF72OT8q3kjj0THymNSIoxMV9jDpmM/cVqwSUqEmafoapOYI7mwYFONMPP+NrFpoXXYoWjQ
                        5w78Oj5tameT1Ch5HmMTm0591ZV6CmjTOuiJ/kZzfvEPQR+b9L2faVOVNDJdoa9NYs5kp9Yz0Mem
                        M191tR6BRjZpRj3XpjIVy3SFxjaJOVP6TLGRG9l04qtObVOdODJdobNNj+1Gio3TySbNrJZGEikW
                        mDaZ2G2iQnlkusI+NgW86sScyYTWpui4rWx6P80mbdzoPK1s0j7KBc7nmmrut7LptFed9ukoFWje
                        XG+boJeT0iYpFWieZiebtA9zcZuUUaVUv/M0O9l02KuunPi0yQpuGM3Z5XjfzCblARQ5JJ9hiiWa
                        97aVTf4/pBn1a9puUM/6Zjad9KrTHF1SzG42KY9gvZZqo6SkpE12QKMUDNnNpohXnZhDxU+Sk7Gd
                        TZqB77LBIPUyyXSFzWwK+Fsd5lswVQklJ1M7m4551akSJmXqZ5Nm4mKHtTyGFAw1z7SbTYdcTu8V
                        badNLtSeQrJCNbRJM3K581oaomSqeWXb2XTE5VRT9Y42fQyw6ZE8Q03TO9qkmdnWTYkREqN1tOmA
                        V13RZC1t0gxtKyeKosFa2vRHgE2pl5Pm1KrmkukKG9oU8qoTc6qQ/Jmxmtqkmfo2NeOX7VWmK+xo
                        0+aXk+LQ8kIpg51kk2bs26Slr5hJXatMV9jSpq0vJ8WZZUXSJ6NNr6iXPbnVrjZp5jYW5I3i6xuP
                        wq3Oy9rTpo0vp/tHJtmltrVJM3it4ysm92qp8642tWnby+n+iUl6p31t0kxe6QBLqW3odF7VrjZt
                        ejndPzDJr7SxTe8RNok51gsKif07jW3SjF7mDO+HluKVzvNta1PMq07MsZwyIxrtbJNm9iKneD+y
                        VG90HnBfm2Iup9DfNVdE6me0tkkzvLWm5MCBIZwKnSfc2KaYyynwWiguU3ObNNNbe0qNu0Gf85Z2
                        tmmzywnv8wua26QZH64Ten/fOmW6wtY2BV1OYs414f5RhWzvG/FMmzTzK4iIit3dvU2ZrrC3TUGX
                        U8CB3j8pgZXZ3iZNAeauDNz/7a+f9ihz3tDmNu1yOeF2VkCbNA3gdLqfUjbpcp5zd5uiLicxB/sB
                        xTEhq6RNYZeTmJN9ByRxZJXzoNvbFHU5OR6sYlNok7Qp7nJyO1lFQNmmyXnS/W2K+Q5xP500Z4Qt
                        kjb9TZRNLoerOaIHtkfapG0hWyfNP1olG/U4z3qCTXGXk/kL06rf0YmukTapa8jVSbOX7FTjPOwR
                        NsVdTjaddL89GF0ibfqHkF/6bNZJ9w99CrpE2rRShJakTOgKadN3Am1aPGelTIJukDatNZGikzYQ
                        ukBlZJmucIpNcV8Rf97dFfE7+EObvhNpk/bq0JzLyvox0Ka1LhaQ+0l0f5fTrl6jwXngc2wK+Uc1
                        V058ZW10d1+gTT8SbNO9Mw8VNRbatNpGjE+rCdDN6dPLdIWTbIr9e91ljV+IWDMT2vQzCTZ9nv/H
                        Fs1J/Aa6toUZZLrCWTaZznRRAfueYpgY1d4881k2perkBrq0lfJkusJhNiW961wR89BO0KbfQLtB
                        m06ySVNJDdCNLVUn0xWOs2k7ncQ8MaK5eerzbNrtXYeu6zu0aQbaDxViHtcN2mRtBQ66rMXeZLrC
                        iTbtpJOYh8XUNs99pE0bvevQTf0IbXoCWpK7hP4jQVpo0zPQmpgOBQRt8mgGCLqm5c5kusKpNu2h
                        k5jHRFU2T36sTVvohO5ovTGZrnCuTRvoBPxN89bCZLrCwTbV10nMI8L6mkc/2abyOqH7MdQl0xWO
                        tqm4TmKeD9fWY7rC2TbV1gldjqWs+QqH21RaJ3Q3v0GbXlFXJ0FX8xu06SVldUIX8zu0ybekRIp9
                        sUlZ1HyBDjbV1EnQrfzOsIZvYVNJndCdTKBN3j1lIeaZkC09Sd/Epno6iXmkAG6nf/LxXWwq97ZD
                        12Hr6MnH97GpmE7oNkwVPfv7aCObSr3tBF2GqaFn6TvZVOl6Qjcx565NH558fC+b6lxP6CKeYEzf
                        zKb43wN9D0HX8ARj+nY21XjdiXmKGAZtiqksFHQFT7Gl72hTAZ/QBZiakacf3dMmtE+CHt/Uy+Pp
                        R3e1aenf1mlg051X3fMP7mvTG/DzcfTgFwzLo9DaJtgLDz32FZbwzW16gwgl6JktfVyFp01/c9uD
                        Ty6vR0HPaynj6kNp0zdeOvD1j/1xvE3jMvvj6kNp00+8Pv3rru+BnvIFVyPK5UfSJiUNbLq4pl/8
                        oA1tUuIgU3mb3laT0yYlDjIJeoblKV99GG3SMXrYNNfp5UfRJh1dbJoMeiM2bdLhYRN6hqVR5c6H
                        0CYdDjLtYtMCtEkHbbqCNqnweNHRJvIVD5sEPUQctEkFbbqENqnweNEJeog4aJMK2nQJbVJBmy6h
                        TRoGbbqENmlwsQk9RCC0SQNtuoY2afCQiTaRr9Cma2iTBtp0DW1SMGjTNbRJAW16AW1SQJteQJsU
                        0KYX0CYFLjLRJvIF2vQC2qSANv3nNv9FRy0PbaJNbgzaRJvcoE20yQ8nmwQ9hwHa5AZtok1+0Cba
                        5IePTLSJ/A1tok1+0Cba5IeTTTt/iYA2uUGbaJMftIk2uTG8bHqgJ1mHNnnhZpOgJ1mHNnlBm2iT
                        H7SJNvnhZtPGn4bTJi9oE23yw88mQY+yDG3ygjbRJj/cZKJNxPNfKEePsgxt8oI20SY/HG0S9Cyr
                        0CYvaBNt8sPRpm1fdbTJC9pEm/zwtEnQwyxCm7zwtOmTPQ4E2uSFp027vupokxeuNgl6mjVokxe0
                        iTb54WrTpq862uSFr02CHmcJ2uQFbaJNfvjatOerjjZ54WyToOdZgTZ5QZtokx/ONm35qqNNXnjb
                        JOiBFqBNXnjbtOPlRJu8ePe26YGeSA9t8mLwVUeb3HC3acNXHW3ywt8mQY+khjZ54W/TfpcTbfIi
                        wCZBz6SFNnkRYNN2lxNt8iLCpg/ooZTQJjcCbBL0TEpokxsBNu32qqNNbkTYJOihdNAmNyJs2uxy
                        ok1uhNgk6KlU0CY3Qmza63KiTW4MXk60yY0Ym7a6nGiTG0E2CXouBbTJjSCbdrqcaJMfQTYJeq77
                        0CY/gmza6HKiTX5E2STowW5Dm/yIsmmfy4k2+THaX060yY8wm7a5nGiTH3E2CXq0m9AmP+Js2uVy
                        ok2OxNkk6NHuQZscibNpk8uJNjkSaNMev4+eNjkyAnUS9HB3oE2ORNq0xbuONjkSapOgp7sBbfIk
                        0qYdLifa5EmoTYKe7jW0yZP35pcTbfJkNL+caJMnsTbV14k2uRJrU/l3HW1yJdgmQc/3Atrkysfe
                        lxNtcmX0vpxokyvRNhX/lfS0yZdom2q/62iTL+E2CXrCK2iTL6P15USbfIm3SdAjXkCbnAm3qbJO
                        tMmZeJsKv+tokzPvnS8n2uTMSLicHughn0GbnMmwqey7jjZ5k2GToId8Am3yJsOmqpcTbfJmNNaJ
                        NnnzR4pNgh5zCm1yJ8WmmpcTbXJn9NWJNrmTZJOg55xAm/zJsani5USb/EmyqaBOtMmfkWSToAf9
                        DdrkT5ZN9S4n2hRAlk3ldKJNAXzMsknQk/4CbQpgdL2caFMEaTYV04k2RZBnk6BH/QnaFMFoejnR
                        pggSbRL0rD9Cm0LIs6mUTrQphMTLqdK7jjaFkGmToIf9Dm2KIdGmQj8QRZtiyLyc6rzraFMMqTYJ
                        etpv0KYgMm0qcznRpiBSL6cqOtGmIHJtEvS4X6FNUaTaVORyok1RdLycaFMUuTbV+KITbQoj16YS
                        7zraFEby5SToed9oUyDJNlW4nGhTHMk2CXpe2hRJ9uUk6IFpUyTJNuHfdbQpkGybBD0wbQpkdLuc
                        aFMk3S4n2hRJ+uX0ATsvbYok3Sbwu442hZJuk0DHpU2hNLucaFMsvS4n2hTLe6vLiTaFkfOPGpTS
                        iTYF8CfIo68IbnDa5AvUI/jlRJu8GGiJ/kVgHdAmB+qI9BVYEbTJRjWRviCoNmjTOiVN+sID1Aht
                        WqOuSV8AtUKbFkC78hrBFEOblAy0KPfAlEObNGyi0mfU5USbbvMRbYgKSEW06R5oOdQIoiXadAO0
                        GUsgiqJNr0BbsYoAuqJNlwy0EwYe+XXRpudgv7HETn5jtOkZaBfsSHpntGnKQJvgQnpttGkC2gIv
                        JLs42vQrqG/njiC7O9r0MwMtgCuS3B5t+pGzXPqcfjnRpu8c51L65USbvnGgS5+zLyfa9JUzXcq+
                        nGjT35zq0ufky4k2He1S8uVEm452iTblgj7tcDLLbG4T+qgTkMQ6W9s00CedQmKhjW3q4VLq5dTX
                        JvQh55HXaVebBvqIE5G0VpvahD7gXNJqbWkT+nSzeWQV29Cm3X94QI9kVdvPJvTRIsjqtptNJ32f
                        7n0kqd1mNg30uYJIqreXTehDhfHI6beTTQN9pjgkp+FGNjWWKetV18em7PP79DTJANgkKR23sSnv
                        4B638iQr9elWKCtNbEo6u4cmU65OKTX3sClBpoWHP9Umyei5hU1FT2qk6pRRdAebyp5Sqk4ZTTew
                        KfCEpG4096x3ON6muO8Y8PhrUqZOCWWfbtOofTZh8WhTAEGnJW4BE23yC/2Us22Kkck1YqJO8X0f
                        bVOETN5fVI7I+IRHeOEn2/TufyDinzJPp4Dwv3CwTbscR5pN8a+6c2366H0Wj6CgI82mqAn+5Vib
                        vGWSuKhpOgXO8JVTbdrqHLJsCn/VHWrT+0YuJf6EX3TtZ9o0tjqDvHedBM9xpE2uhyMZiZNsin4w
                        TrTJU6a/ciJnveuCxzjQJk+ZHjuGvkBipzjPJsfHPOdb87+SY1PwVXueTX7VPzJjjxydYoc4zia3
                        3jMvpr/J0UlCZzjNpk1qD41Om5wYXq0jwqfoFDrBWTZ5ySRbp8eNdpZNOzy/8fFpU6XT2H4A2HQn
                        2TTqP7wpE9CmMkch0BkSbIoc8BybfH4/KnoK2lSDI2TKeNcFpj/GJo9jEPQQbyE/aEObtJwiU8K7
                        LnDMU2yq3bIGj+fimrjsh9jk0PEDPcM3wnWKi36GTQ4HgB7hB6JtkrDkR9h0lkzhl5OEJT/CprNk
                        itcpLPgJNtnLR0/wK7QJxnkyReskUbEPsOlAmYLfdRIVe3+bzMWjB5gSalPYyNvbZJbpgZ5gDm3a
                        sXVBD/AE81OCGHp3m6ylR/VqhzalY/2mpqhaPaBNmzWe/ROYKkagTkGR97bJWjg6/zW0aau+0fFf
                        YH1WLpCYxFvbZKw7qNIy89GmxLKDGvUkzKaga3lnm0oW6orxeaFNaVWj49/iPcomCYm7sU0V6yw2
                        JG26yyjYZrUpadNdbG2i0yeNmVzAtjbZHlpBx0+akzbdo2CXMQTpJBFZd7XJ1jE6vQraFI3tewdC
                        mgzD9uDQpte8m5pEp1cSYlNICXva1OlqegvSKSLonjbV6zGUEWGTBATd0iZbu+j0C9CmQN6rtRiN
                        7fGhTVfYPmtCp18iQqeAmDva9F7tkUyANpVsFh1+kUGbChYr6PSr+NsUUMWGNlV7IHMwPUO0KaTW
                        gAazeKdNAZgaRIeHDZ5TxnY2jWKP4yaT06YpxfrbZnTaNMH0gAo6PXD2lDp2s6nYw7jR8LTpN0yP
                        p397O01Pm37D1B46PHj8hEL2ssn0cAo6PXh+2uTZJjq8B7SpSJmCDu+B6XGiTX5dosP74GqTeKfb
                        yiZLc6V/xeV9TA8UbfJq0r05ELSpQpHo8F6YHina9J1SxW3ZQvQjtpFNpqcSHb5IDbTpG5baBB2+
                        SA+06R9Mz6SAw3vyTpvslKoNiemxin3Ietjk3dq+TdCmL5ieSGz0UlXQpr8xtYaNXqsL2sTPwf3K
                        oE180f2Ml03ezexiU6XO8NAmE8NSmSCT16uDNlWqrAK0CdYeMngQgzaByhNg8DCcbHLuZg+bKj1/
                        NTA9X7SJNv2E7Xd/trbJ9CA6F1YF2gRp7oHLHYnpCaNNq+BiFy6lsU2mx9C5rzr4XE6+mXawqdLT
                        VwjalF8bLHU4LpeTb6QNbLK1hkpdvZeuNpnaElTq6sXQJtr0Ex6Xk2+i+jbZOgOFzoE2qTHZJKDQ
                        G1QTUVB9mwqVVQ7apMT2/GEyp0GblNCmsHIa2mTq6pBfKBfUDm1CdlUQ8+Xk21B1m2x1+XZVENqU
                        WBckciq0Ka8tSORUrJeTuKYpbpOtLN+qamK9nFzD0KbNoU1ZXSESZ2N73mgTbXKsqJNNxgcPkDgf
                        2nQXm00CSLxbR51sshUlgMTbleQahTZtD226xyhUVF1sLblGoU3bQ5vuYZPpr/zAGGhTfE1dPm2i
                        TfcYtCm+J9cktOkAaFNwSd49laZMS7TpAEaVlmjTAdCm2I4+d/q0yfQrVV2D0KYToE2RFdEm2uRX
                        UTebBm16gfX3qyfHhUKbAhtqZ5PhHneNQZuOoEhNdW0yykSbaJNLQV84/tej/MSgTdcYbZLkuFho
                        U1Q/tIk2+fXT0ab1m9w1RVmbjDLRJtrkUU9ES/Wp0RNtOoNRoqeqNv1Jm1TQppB2mtq0/F81XVPQ
                        pkMo0VNVm95pk44SPVW1ySqTpKYtwOrj5xqCNh3CoE3PoU1KaJN/N7SJNvl109em1dvcNQNtOgXa
                        9JSPtEkLbfKuprNNgzY9w2xTZtga0Kan0CY1tMm5mdY2LT6BrhFo0zEs9SSuEWjTMdCmJ9CmBd5p
                        0xyzTB1tWnoExTUCbToG2vQE2rQAbXoCbcpqTVwT0KZzoE1TBm1agTZNoU1L4HuiTefwEd4TbTqH
                        ldp8E9Cmc6BNUz7SphVo0xS7TLSJNn2DNi1Bm6bQpqzenH9zMW06CH1N4huANh0EbfJphTat9Sa+
                        ASraNGjTGrRpAm1ahDZNoE2L6Gt6+AagTQehL845AG06CNrkUgptWizOOQBtOgja5FIKbVoszjlA
                        RZscZKJNtxDnALTpIGjTBNq0CG2aQJsWoU0TPGzy7mkL1DY9nAPQpoNQ2+QdgDYdBG2aQJsWoU0T
                        aNMitGkCbVpEa5N7SbTpIGjTBNq0CG2a4GFTyy9f0qYJtGkRrU3uAWjTQdCmCbRpEdo0gTYtQpsm
                        0KZFlDaJe4BjbfKvqj60aQJtWoQ2TaBNiyhtergHoE0HobTJPwBtOgjaNMHFpo5/qaNNE2jTIjqb
                        xD8AbToI2jSBNi1Cmyb42PRIy1uGj+jH7VybJC1vGXQFBQSgTQdBm8yl0Ka14iL6Odemhp+G0yZr
                        KbRpsTgJCECbDgJeD206CHg9B9skaYGrQJuspdCmfxnwdmjTOdCmGU42/ZUWuAgqm0ISVLRJ1Up2
                        YYWhTeZWaNNabyEJTrZJ0hLXQNNbTDe06Rzw3dCmc9B0E5PgZJu6feKEr4Y2nQO+Gtp0DopmJCZB
                        RZv+9LIpqLOiaB7CoGYq2uT1xXDa9JSgCEfb1OtVR5vm0KYVFDZJUATadAy0aY6bTVGtlaRAL7Tp
                        GBS9REUoadNw0ykvM54CtdCmY7jfikRFONymsN7qoSgtrBXadAqK0sIyHG5To1cdbbIXQ5u+cb8T
                        CctQ0ia/LxE0etVV6IQ2ncL9TuIynG5Tn1ddhUpo0yGM241IXIiaNt2vBtldKWiTQzW0SV1ZYIjj
                        beryqrvdx6fAELTpEG73IYEhatrk9nMGwe3V4f7zF5mipk2ef6nrcTnRpgtok5LbNklkigY2hfZX
                        hRptFLXpnTbpuN1GaIqiNg3PyykzOIj7f20JjdHBJslMjuF2X7Fd0KYjuN1XbIyiNrl+Gt7gVVek
                        ihY2SWp0BEWaoE0nMIo0UdWm2/3cIjU6gNttBefoYZOkZs+nSg+06QSq9FDVJt9PnA5/1d1+9KKD
                        NLFJcsMnc9emyG+U+0ITm86+nMo8U2Vtuvu80ab7NoUH6WKT5KavWVV4krI2eX4z7+GX012bJDxJ
                        WZu8P3GKrxJGnQeKNu0PbXrNcNYpOX69oiQ+Sh+bEsrEUOhxqmuT96fhx15ON8cP/9LlW2WbvD9x
                        OvVyGoXGb2TToZfTe6HpC9t096G7zYfsCVIodDW1siml0KotpQxf2Ca+6u7wsdLsrWyS9BHKlJQz
                        emWbBi8nt45y0vSySdJniKbWc1TZJv9X3XmXU63HqJlNSa2mMWo9RaVteufl9IJiD1Fpm+4+eW0v
                        p7sFZU1d2qY//G0663KqNnRpmwI+cTrrcqo2c22bBi8nj3rSAvWzSQBzBFFu4to2Rbzqzrmcyl1N
                        HW0SxCDIcvISFbfp7uOn4oGYBNeN5EXqaNMh77qC0xa3KeRVd8a7ruDV1NOmIy6nirNWt+nuE9ju
                        crr73wlSR61uU9Dl9MAM48h7waupq03bv+tGyUHL23S7Nh2CmcaNmnOWt8n9B8iPuJxqXk31bYp6
                        1e2tU82raQObbj+GSpKLxnSSnKuvTTvrVHXE+jaFver2fddVvZp2sCnsctpWp6pXU2+b0tv2oe7T
                        soFNca+6PXW6/XTlT7eDTXGX05bvusLDNbdpQ51ujyb52XawKfBVt59O9x8tQLgtbLrfoB7BjbVE
                        6cG2sCn0ckK0ntEEIh1t2kqnUXuqPWy6X+IKD+BkYT1A8u1hU+zltM9n4vdHEki+TWxSPJQH61R+
                        Itq0j06KEgSTcBObgl91W+ikeaJAEXexSVPloTophhFQxF1sCr+cyuu0wyzb2DSa66SZRFAht7Ep
                        6mdXNtFpj0G2sSn+VVdZp6EZQ2Ax97FJVegiD/CMPrPjcu5jU8blVPS/2e0i00426To9SCfd4MgJ
                        NrIp5XIqqJPyKUJG3ckmZa2roMe0TS3IrLSpuE7aoaFhd7Ip51WHPpGf2UqmvWzSVruMoCddnRgc
                        fCub0i4n9Kl8Q/34gPPuZZO63XXQoy6N+wAH3sumvMupgk4ftZEFnXgzm9RPqwEBz7rhA7CZTZmX
                        E/h09HEf6MPZzqbRRKeFOQV9NvvZFPHvkBc8oQWZ8O+5DW3KfdeBzmiboL+wn00rj60FyR9xk5i/
                        s59N2ZfT588fcucbKxk/oU/lCxvatNS2jczx6id8zoY25V9Oie+RsZbvA/pMvrKjTYuN7+BT6XCv
                        2dGm5K8S5B3ZcjT0gXxjS5sQ77r4QxtFcynY06b14sv6dPdfVp0g6OP4lz1tQl1OcT4ZEgn6ML6z
                        qU0JP0ee6ZMljqDP4gc2tQn3rgs4wHJyL7OrTcB3ne8ZGj5fKifTvjYNsE4+52gOIehz+IltbSqg
                        k1UohwkEfQo/s69N6HfdPzwW0797bC7oM/iFjW1C/r3uJ/T//d5rZ/QR/MrGNpV4131Dbqd23BR9
                        AL+xs01F3nX3jzd3NwBb21RPp3/466eUI2aTB7r939nbpqBz2gJBlz9hb5sa6yTo6mdsblPZd11L
                        mba3qalOgq59zvY2DfTBUqbvbG9TS53QnT9jf5savuvQjT/lAJva6YTu+zkn2NRMJ3TbFxxh00Af
                        MGX6yhE2ddIJXfUlZ9jURyd00dccYhPox38p08+cYlOLz8T/stcUyzE2NdCpxu9ouuIcm47XSdAF
                        v+Ygmw7XSdD13uAkm6w/6FgaQbd7h5NsqvNTLE1lOsumc7/sJOhm73GWTafqhK71LofZdKZO6FJv
                        c5pNJ+qErvQ+x9l0nk7oQhWcZ9NhOgm6Tg0H2nTUlzEF3aWKI206R6cHukkdZ9p0ik7oGrUcatMZ
                        OqFLVHOqTQd8Li7oCvUca9P2Ogm6wAXOtWlzndDtLXGwTTt/8iTo6tY42qZtdRJ0cYucbdOmbzt0
                        a8scbtOOOgm6s3VOt2m/tx26Lwvn27SXToJuy0QDm3Z626GrMtLBpm2uJ0H3ZKWHTXtcT+iS7DSx
                        aQOdPqArcqCLTdXfdoKux4U+NpX+UWB0N040sqnu607QxXjRyqaarztBl+JHM5sKXk/oRjzpZlO1
                        6wndhi/9bKrkE7oJbzraVOV19wHdgzstbSrhk6A7CKCpTfDXnaDnD6GtTVCfHujZg2hsE8wn9Nhx
                        tLYJ8fmToEeOpLlN2b95FT1tMO1test74Ql60HBo098kfHuBoGfMgDb9w6BKdmjTd4KEEvRcedCm
                        n3AX6gN6olRo02/4qYSeJB3aNIXvtyVo03N4J2mhTS/gjaSANt2DHt3hvk2EEEIIIYQQQgghhBBC
                        CCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQQgghhBBCCCH/8P/T2g3wTNSy
                        bgAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMC0wNy0wNFQwMzo1MDozOSswMzowMFesjGwAAAAldEVY
                        dGRhdGU6bW9kaWZ5ADIwMjAtMDctMDRUMDM6NTA6MzkrMDM6MDAm8TTQAAAAAElFTkSuQmCC" />
                  </svg>
               </a>
               | 
               
               <span class="icon twitter">
                  <a href="https://twitter.com/i_amanchadha">
                     <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                        viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                        <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                           c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                           c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                           c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                           C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                           c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                           c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/>
                     </svg>
                  </a>
               </span>
               <!-- <span class="username">i_amanchadha</span> -->
                | 
               <a href="/cdn-cgi/l/email-protection#11797851707c707f3f7078">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                     viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAW4AAAFuBAMAAABTjO+8AAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAALVBMVEWxsLDGxcW4t7esq6u+
                        vr7Z2NiqqamxsLCvrq7Ozc2ysrK1tbWenZ2dnZ3////zevNgAAAAAXRSTlMAQObYZgAAAAFiS0dE
                        Dm+9ME8AAAAJcEhZcwAACxMAAAsTAQCanBgAAAAHdElNRQfkBwQDLRvUSpUpAAALt0lEQVR42u2d
                        PW8jyRGGR7Lk3XXEyNqQwAVKBewtiHMkwAfQoQ4L3B02ImCs98KVuFZzFfECm3Am4A7YDXmR/sQF
                        /gHO7+94R6Somarq7qr+mGra/YZifzzsfmc4XdNdappcOvjixZcX2VrPpj+az1r8QxtDqp/MRmfa
                        IDI93WKbxUgbRaS3D9zmUhtFolfmUUttGL6OOtjm+kIbh61Jl3t/nPLK9LXUBuLpAGDvi1NWkHs/
                        nPKtwVprQ/l1QGDvg1NWFLf5URvLp28NrbUCy+sfzJB6MUqD/SoeRagkk/M0nkOqFA+Rx2+H5zY3
                        8dzfK2CncIrGcBtztYfuvtdtJPdHJe7ZXtrEmPdx2MdK2OZ6P+1tzEUU9+/VuMdR3E/UuM/+L7lP
                        9pR7um3lry+G0iQp93lUKxL9rnJX7spduSt35a7clbtyV+7KXbkrd+Wu3JW7clfuyl25K3flrtyV
                        u3JX7spduSt35a7c+809Men0rnJX7spduSt35a7clVuiOzXuZQz2QyMK3POLcOzjHd/w3DEnBaa7
                        xgH3z1+nE2h6xx3u8McmFNYN4U55dIkOd6hTpkaZO8wp3QaUuEOO93ZdosUdchC86xI1bvmvT7+6
                        GrfUKX2X6HFLndJ3iSK3zCnoWJoet8Qp+EyxHrfEKSemIG6+U4jDi5rcXKdQJ881ublOwS5R5uad
                        SiePuOpyc5xC5yfQ5eakIaFcos7td4rlILQ2t88ptiwW2tw+p9AuKYDb7RTrcXl9bpdT7LlO9Lld
                        TrG5pAhuu1McSRVK4LY5xZURpwRu8y+6ht0lhXDTOSOcqTfK4KaSi7jzJpXBTTnF5RLE/eeEiRXO
                        BNzYKX8wEu6JSad3NPeC5RSLS+ZTNe6XNBFwynPLMOtxnx3SxXtOsbjkjer7tE9k8a5TLC5ZKL8H
                        pHvpOMWSg22kzH1EV9g55XhFfv6m0X7vSieP3CXL+97+sS63JVnn2jXcm9yFytxkctSHAadza82a
                        ArgtTtnQ3FEfbRMAanPTcB/aT55Rn8xvC+E+WFF03YI9bWvpc9MuXjb4lUir3Ztafe7mlKjzgWZ4
                        XMkVwA1fkd0D0j9Ky8bDnVF4vw9l5FvKP51IRQnclJPPmu8cLimEm3jse0fYft1ppQhuYsl+iROW
                        9lYUZXDjZc0NujX0V3CFcCOnzBv4TcYlcjeHPu6/91sphRvmDUbcoHgx3E8qd+Wu3JW7cv9Pcpf6
                        ezn1cLdBwQK54ZoHcy9K5EZrTMxtXhbIPTV+7h5jGdx4DUxxz0vjfrZicZuvCuM+NTzuzpqnBG4q
                        JktzP64xC+C2RDYp7sc1fQHcd4bPvYuh6HPTkXsb90PMSp2bflNi5X6IEapzr4TcW6dYuH/9Tzr9
                        28Vt+zdydu6NUyzcv5l0eufgPrBVsnNvdl0rc08CuHsvUXS47f8pz8XdOkWV+8heycXdvp1S5Z4E
                        cn+uq8n9yVHJzT2/UOQ+dFVyc5srRe63Edxmosa9cFZC3H+iyw3P3dfio4f7jN7Zps09e+LjZp2D
                        GZr7hhFn45w7Gph7MeLEB0+K437DimsyztUN8BzbUbufisFNVQXcGUV0PmJyExsQNLnbfYw8brxV
                        RZF7E2hlceOAnCL3uYAbhbb0uLfhYSY3DCW+DOaQ6hQCiriRU4YacNjvWMgNw1uR/9+WKzjPu4Af
                        m/vA1kJWWUeLzY1CLmM5hViwz3UANwxxhSR/EQrOcWdDoIAbBosYh6oj5RgpAbdj1vII9rfsfCbh
                        hgGMzE6B89tLryDiPnK1lFzO60nEjQJ0Sz6FWK+cJDJuuJszo1Pg3IJkYULuQ3drCQWuJZicTciN
                        gnRnPAqxfgL9zMDnUm7olJh0hg7Beb2CBcTc8KhgHqeA0VncRnOjPcwzP4VYn7x9yLlhWAKPRbTg
                        nN7gInJu1OqVl0MoNDKjJNwogPUmMfdzRvsh3JzxiBAMSC6oQiHcqOWbJqFQWI8clSBuFMBK6RSe
                        C8O4YQAroVPgXM7pYmHcKBqzaBIJueQ8KTcKYDGc8jNMxUsJOtAWYArlRnvhRh6g1+1AXvtKoSyJ
                        toKh3CiQNG+c2q4C5u4hR4HfcXJutD/LGTLcrRXdKw3oPntwKZwb7YdzDeUdAwXPoeNLhnOjMIHD
                        KV3bjqyl0DWztrcYwY2cYh3Knm3tt0zoEldgKYYb7XYaWwqe9ErZbplw/pyXQgw37okuBn8CR/Qo
                        QJcsXV1HcaPdfOTMop9A+jEMus4dVIrjRjue1kShE1iIdAq6yi8ycqOgEuFJIucO8RiGrhVPx5Hc
                        aK8Wcgq5nwIv7aDjfGGCWG6062kJCtD7V2agFHTJ4sLTbyw3ChkAp1gyM4EgALpOZr5uo7nRwfVL
                        56e0UybOT/NwoxHtVrHv+euWglc3IyQTz40c3LmDOfb8uUoxFiEJuNEd4/FeYHOJuxQnPJCA2+4U
                        +/7hVkuLl1iL7BTc6Bdxmw3IsX+41fbOE+KSRNzIKZv7wcTNvbnzoLq80EASbrxXb9bYzyL0nYJ+
                        lzguScWNwgef72ToieP6G8Ip6Npg7mxJxI0W4jf4xMoaz8CPREKQQbnxNqi/wT+0q7g7+Ef4ddn7
                        cVJxk2mlep5oC5Fn4uCXG5b7mQdpfF/Kc63ydxEl47ak0YMD6Z6WsQK3E2l3uTmnRbCjJSG3y7zn
                        nGmRvO1PyO0wb/embJ+WtaCvlNz0UVQDbspUXrh7iXazJOW2Hdcb9UpZMjXL9oQk5bY4BT7gTclS
                        S1FPabnJ46joAY8MTQj3PCXmJpxCLAOIox7S/SCJuYklDrUMOEGlpP2k5j6GQORiETlFvE04NTcc
                        SctiET12S984J+ZGPDNLQbjMkb5xTsttWWhGlRyCG43irbUoWsbP2L0k50YsrtoobHLL7CU9N5z7
                        9wlLZ+SGI+j5LRHNTkZuMQf8jZL8ZibknojnXV4jAzfjFRUUCiAuh+cOYgj4rqm5V6Ai77k0rFZC
                        btHL9Ueh5971sNzB/Qd+31TccL75kZDAmmm4Q0etIaIu6+G4Ud9jNjZ7M0gO7jtQSXZWMKh2Cm60
                        nUuEjUOG42G4Ub/nMu6g752A+xRUkZ89Dmghnlu4cZPSs4l4xqK5j+V9YuG0qtm5p6BC2BZ2cSux
                        3Ik2gqNZG+XlFvdnE+swRjrukyQuCWgpjjvhwRLh4Zoo7qQHeWRjEMXNOdjEl6i1GO7EB9VEB/Yi
                        uJMfDJSMQwR3+oOYghbDub1HaQPEn8FwbtBHkoPG/OPiwdx5DnazWw3l9hzLD9aEOYuh3KD9ZIkL
                        uCkoArnzJYpgthzGnTMxB28mw7hXmVzSypn2JI47b+IZVush3LkT/XBmM4T7LqNL7scFgFPjEsCd
                        Pz0Rowc59xCJw+CMpuD2txkv/9iIuW0J6tLKm1RNyg2DmF+xMOQ6hVyR3L72UsmXqFHIPVz6RE9P
                        Mm4YnsqZrnLqnFkZN2grWVYISnCM+mEJETcMYo5ycrt7k3DDwEPq/DhQrtmVcJ8M6BLfOAm4QQAv
                        cVYfSo4e+dxDu6QVmOFOqJPP/dzaRj7Zx4rNDYJ3GbKEUbL2yuWG33w2CDaa5V0wj8v9yVI/t2zj
                        xeQGgbuBXEL1fCHifkv+dRCBmX4v4abrDiRyzFjcIDyVKTelTYdU7yzuiZ5LWoGQ4SWXm6o3qMC4
                        LXncIDw1QP5sqCNMwOBe4W87tMDy/pLDDerkz/pNCYzd2s+t75JWiMLLfQe/qY7grPu4/1mCS1qB
                        8Tv1cIP5UcP2HGH3cI/1uN1H2N3cw/yvCZtOQ7mzBQN5ch1hd3Kf63K7jrC7uIf73zU2nYZwK7uk
                        lfUIu4v7XJu6sR5hd3FffV2CJmLuslW5K3flLk+Vu3JX7vJUuSt35S5P+8p9vafcc2/y1DJ1RWb8
                        Kl8fmo/aCEE6sy/1i9bal9S4UDXu3Nelqn2B/Z02RIBmjT1pY8HahC5PtDHE2mxf8mXeLU4PkeKn
                        8U0Nir2LFL9eabMIdN0JcB//5cW+6JcN8X8B85vetwnigQ8AAAAldEVYdGRhdGU6Y3JlYXRlADIw
                        MjAtMDctMDRUMDM6NDU6MjcrMDM6MDDsnuMrAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIwLTA3LTA0
                        VDAzOjQ1OjI3KzAzOjAwncNblwAAAABJRU5ErkJggg==" />
                  </svg>
               </a>
               | 
               <a id="theme-toggle" onclick="modeSwitcher()" style="cursor: pointer;">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                     viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAMAAAAM7l6QAAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAACYVBMVEU/PzpEREBBQT1CQj4/
                        PztAQDtHR0NJSUU+PjpISERDQz9AQDw5OTRFRUE8PDhCQj1CQj89PTlKSkY+PjlNTUlLS0hEREBD
                        Qz9aWleHh4WtrazBwcHCwsLCwsLCwsLAwMCurq2IiIZgYFxXV1Sbm5rFxcXCwsKgoKBkZGFERECX
                        l5bExMSPj45LS0empqWpqahUVFCnp6axsbFTU09CQj6lpaSpqahISESRkZCNjYtUVFG/v7/FxcW7
                        u7vExMVhYV6ampmTk5FXV1S3t7eenp1VVVHCwsOYmJd3d3XIyMjCwsJdXVqEhIKrq6uGhoSnp6aX
                        l5aAgH6srKzAwMBdXVq8vLzCwsOZmZhNTUm3t7bDw8PCwsKYmJexsbCYmJawsK/CwsJOTkq2trXD
                        w8K9vb1bW1jBwcK9vb2pqaiXl5aCgoCvr66AgH6jo6OGhoNYWFXAwMB9fXvIyMjGxsZeXluamplM
                        TEi5ubmcnJteXlrCwsLGxsaTk5FDQz+dnZzJycljY2CJiYe+vr5bW1hUVFCcnJuVlZRGRkKmpqW4
                        uLd8fHl/f33AwMCioqFFRUFQUEyurq6wsLCFhYNkZGBSUk9SUk9hYV6JiYenp6bHx8inp6ZKSkZP
                        T0unp6bExMS6urm0tLSzs7O4uLjExMSioqGMjIrExMTKysuVlZRFRUFiYl6hoaDExMTIyMicnJtr
                        a2hhYV6Li4qxsbDDw8O+vr2zs7KHh4VlZWPHx8fGxsbCwsLJycnIyMjDw8PKysvExMTKysrMzMzL
                        y8vFxcXJycrFxcbGxsfHx8jIyMnExMX///9/oPL/AAAAuHRSTlMAAAAAAAAAAAAAAAAAAAAAAAAA
                        AAAAAQEYUJzK4+3kzJxYGSKE5+qSIgJe7GoGlqYMprcLAZapAl1uH/H70vMkhWYP1ZoW7G5G/fMb
                        UbpinWtbrMsd1OVuBtfn7m3IbMnlBNTozhzz1Z5sVq5Yt2YW7zf48iCTD9CiH/DzZwWv9Ctp0QsQ
                        rnABqNFKO82sAg22uF0fBwUfV7T7uw0Lp/PYycnV8qtu7/JxASeZ7vGgLh5hqebTrWUhilEqqgAA
                        AAFiS0dEyvO0NuYAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAHdElNRQfkCBYKLR1KuANWAAACD0lE
                        QVQoz23TZXvUQBAA4CG7R09K4ZANFtwKFHcvTrHi0BZ3d5dCKe4uxd1d7jbZJCQFwr9ik1wud83N
                        hyRP3uzOk5lZIE6IUK95i5atWktSm7bt2ncQQHTfg3MVUMdOnRNJ6kRS7tK1GxZSXEhIqH53SWE0
                        HUzp0TMPe6txUS9Vo1nB1N59wi6HivrqNBByv/7Y5gGRgTmU+6DBAufwkF85kCczhoZFwMOGa0Ed
                        MVKjbNRogOgYOahG8dhxJpXHx2DCxOBiY1I0f/IUykqmwjQllwpig+kJWjsDZiYCWop4yQpm6TQ5
                        G+bkVhKaW8LYPJhfR9UFUafcaOEik5ZBebbqFa4SlLf4Ny2vw/oS5CqJRpbavCxr5wpPScPlK0y6
                        ElZlNNJI5bUjtHoNY2th3R9f1/tKCjbINLkRNtWmdy5t5KsY2/yXWltg6zbNqxXylcS376Bs5y7A
                        u+V0JX2N7dlrUmUfArz/gL384KFMDR8+olBWeTQOJH7M4N2vOp6PPIzi6hN8gIyTSASCTp3mz9qZ
                        s43jYQEhAZoI587zPqkXLtrDRPClyzy9aV25eu1602Y3bt66fYen0+/WhNw5x/fuq8we/wcPHz1+
                        8tSyW2w8q8HeKcGR5y8s/gHTTPOffVdevnodyzhE+M3bd7Lp1JeZ1vsPH53/KEwx/xX06fOXqq+S
                        VPbt+4+fQjx1BP8DniGUSqIRNGsAAAAldEVYdGRhdGU6Y3JlYXRlADIwMjAtMDgtMjJUMTA6NDU6
                        MjkrMDM6MDBYVnojAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIwLTA4LTIyVDEwOjQ1OjI5KzAzOjAw
                        KQvCnwAAAABJRU5ErkJggg==" />
                  </svg>
               </a>
            </li>
         </ul>
      </div>
      <div align="center" class="footer-col-1 column">
         <a href="https://www.amanchadha.com/">www.amanchadha.com</a>
      </div>
      <!-- <div class="footer-col-2 column">
         </div>
         
         <div class="footer-col-3 column">
         
         </div> -->
   </div>
   <!-- add permalinks to headers in kramdown -->
   <!-- <script>
      var headings = document.querySelectorAll("h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]");
      
      for (var i = 0; i < headings.length; i++) {
          headings[i].innerHTML =
              '<a href="#' + headings[i].id + '">' +
                  headings[i].innerText +
              '</a>';
      }
   </script>   -->

   <!-- add title case to section headings -->
   <script data-cfasync="false" src="/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script><script src="https://aman.ai/js/ap-style-title-case.js" type="text/javascript"></script>   
   <script>
      var headings = document.querySelectorAll("h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]");
      
      for (var i = 0; i < headings.length; i++) {
          headings[i].innerHTML = titleCase(headings[i].innerHTML);
      }
      
      var toc = document.querySelectorAll("a[id^='markdown-toc-']");
      
      for (var i = 0; i < toc.length; i++) {
          toc[i].innerHTML = titleCase(toc[i].innerHTML);
      }      
   </script>        
</footer>

    <script src="https://aman.ai/js/nanobar.min.js"></script>
    <script>
    var options = {
      classname: 'my-class',
        id: 'my-id'
    };
    var nanobar = new Nanobar( options );
    nanobar.go(100);
    </script>     

    <!-- Scroll bar -->
    <div class="progress-bar"></div>
    <!-- Script used to generate --scroll variable with current scroll percentage value -->
    <script>
    var element = document.documentElement,
      body = document.body,
      scrollTop = 'scrollTop',
      scrollHeight = 'scrollHeight',
      progress = document.querySelector('.progress-bar'),
      scroll;

    document.addEventListener('scroll', function() {
      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;
      progress.style.setProperty('--scroll', scroll + '%');
    });
    </script>    
    <!-- theme switcher -->
    <script src="https://aman.ai/js/mode-switcher.js"></script>
    <!-- mathjax -->
<!--     <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <!-- make mathjax responsive -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
       "HTML-CSS": { linebreaks: { automatic: true } },
       "SVG": { linebreaks: { automatic: true } },
      });
    </script>
    <!-- Copy button -->
    <script src="https://aman.ai/js/clipboard.min.js"></script>
    <script src="https://aman.ai/js/copy.js"></script>      
    </body>
</html>